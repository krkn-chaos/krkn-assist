# Generated by Claude Code
# krknctl assist RAG Container with llama-cpp-python + GPU acceleration for macOS/Podman
FROM registry.fedoraproject.org/fedora:40

# Install all system dependencies in one layer (multi-backend support: CUDA + Vulkan)
RUN dnf -y install dnf-plugins-core && \
    dnf -y install dnf-plugin-versionlock && \
    dnf -y install \
    python3 \
    python3-pip \
    python3-devel \
    curl \
    bash \
    tar \
    gzip \
    git \
    cmake \
    gcc \
    gcc-c++ \
    make \
    wget \
    which \
    # Vulkan support (for Apple Silicon)
    mesa-vulkan-drivers \
    vulkan-loader-devel \
    vulkan-headers \
    vulkan-tools \
    vulkan-loader \
    glslc \
    && (dnf -y copr enable slp/mesa-krunkit fedora-40-aarch64 || echo "Mesa krunkit copr not available, skipping") \
    && (dnf -y downgrade mesa-vulkan-drivers.aarch64 --repo=copr:copr.fedorainfracloud.org:slp:mesa-krunkit || echo "Mesa downgrade not available, skipping") \
    && (dnf versionlock mesa-vulkan-drivers || echo "Mesa versionlock not available, skipping") \
    && dnf clean all


# Verify glslc and GPU setup
RUN which glslc && glslc --version && \
    ls -la /usr/share/vulkan/icd.d/

# Set working directory
WORKDIR /app

# Clone krkn-assist repository to get requirements_krknctl.txt and new implementation
RUN git clone https://github.com/krkn-chaos/krkn-assist.git /tmp/krkn-assist && \
    cd /tmp/krkn-assist && \
    git checkout krknctl_assist_light && \
    echo "krkn-assist repository cloned successfully"

# Clone krkn-hub repository to get scenario definitions
RUN git clone https://github.com/krkn-chaos/krkn-hub.git /tmp/krkn-hub && \
    echo "krkn-hub repository cloned successfully"

# Create and activate virtual environment for consistency with NVIDIA container
RUN python3 -m venv /app/venv
ENV PATH="/app/venv/bin:$PATH"

# Install PyTorch CPU-only FIRST to avoid CUDA dependencies for Apple Silicon
RUN /app/venv/bin/pip install --no-cache-dir torch --index-url https://download.pytorch.org/whl/cpu

# Install remaining dependencies from requirements_krknctl.txt
RUN /app/venv/bin/pip install --no-cache-dir -r /tmp/krkn-assist/requirements_krknctl.txt

# Compile and install llama-cpp-python with Vulkan support (Apple Silicon optimized) - PRESERVED
RUN CMAKE_ARGS="-DGGML_VULKAN=on" /app/venv/bin/pip install --no-cache-dir --verbose llama-cpp-python

# Install huggingface-hub for reliable model downloads (PRESERVED)
RUN /app/venv/bin/pip install --no-cache-dir huggingface-hub

# Download LLM and embedding models (3B model for better performance)
RUN mkdir -p /app/models && \
    /app/venv/bin/python -c "import huggingface_hub; huggingface_hub.hf_hub_download(repo_id='bartowski/Llama-3.2-3B-Instruct-GGUF', filename='Llama-3.2-3B-Instruct-Q4_K_M.gguf', local_dir='/app/models', local_dir_use_symlinks=False)" && \
    ls -la /app/models/ && \
    [ $(stat -c%s /app/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf) -gt 1500000000 ] || (echo "Model download failed - file too small" && exit 1)

# Pre-download embedding models (UPDATED to use multi-qa-mpnet-base-dot-v1)
RUN mkdir -p /root/.cache/huggingface/transformers && \
    /app/venv/bin/python -c "from sentence_transformers import SentenceTransformer; print('Downloading multi-qa-mpnet-base-dot-v1 embedding model...'); model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1'); print('Embedding model cached successfully')" && \
    echo "Embedding model pre-downloaded successfully"

# Copy application files from repository (NEW IMPLEMENTATION)
RUN cp /tmp/krkn-assist/rag_pipelines/llama31_krknctl_rag_pipeline.py /app/ && \
    cp /tmp/krkn-assist/fastapi_app.py /app/ && \
    cp -r /tmp/krkn-assist/utils /app/ && \
    echo "New FAISS implementation files copied successfully"

# Pre-build FAISS index during container build to avoid runtime indexing delay
RUN mkdir -p /app/faiss_index && \
    cd /app && \
    /app/venv/bin/python -c "import sys; sys.path.append('/app'); from utils.faiss_document_indexer import FaissDocumentIndexer; print('Starting document indexing...'); indexer = FaissDocumentIndexer(); indexer.build_and_save_index('https://github.com/krkn-chaos/website', 'content/en/docs', 'faiss_index', 'https://github.com/krkn-chaos/krkn-hub'); print('Indexing completed!')" && \
    echo "FAISS index pre-built successfully"

# Copy krknctl help and entrypoint
COPY krknctl_help.txt .
COPY entrypoint.sh .

# Make scripts executable
RUN chmod +x entrypoint.sh

# Create directories for new implementation
RUN mkdir -p /app/cached_docs /app/docs_index /app/faiss_index /app/rag_pipelines

# Copy pipeline file to correct location
RUN cp /app/llama31_krknctl_rag_pipeline.py /app/rag_pipelines/ && \
    rm -rf /tmp/krkn-assist /tmp/krkn-hub

# Expose port for FastAPI service
EXPOSE 8080

# Set environment variables
ENV PYTHONUNBUFFERED=1

# Set Metal/MPS environment variables for Apple Silicon GPU acceleration
ENV PYTORCH_ENABLE_MPS_FALLBACK=1
ENV PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0

# Verify Vulkan setup for Apple Silicon
RUN ls -la /usr/share/vulkan/icd.d/ || echo "Vulkan ICD check"

# CACHE INVALIDATION POINT: Change this ARG to force fresh code checkout
ARG CODE_VERSION=latest
RUN echo "Building with code version: $CODE_VERSION"

# Fresh clone of krkn-assist repository (this layer will be invalidated when CODE_VERSION changes)
RUN rm -rf /app/rag_pipelines /app/fastapi_app.py /app/utils && \
    mkdir -p /app/rag_pipelines && \
    git clone https://github.com/krkn-chaos/krkn-assist.git /tmp/krkn-assist-fresh && \
    cd /tmp/krkn-assist-fresh && \
    git checkout krknctl_assist_light && \
    cp /tmp/krkn-assist-fresh/rag_pipelines/llama31_krknctl_rag_pipeline.py /app/rag_pipelines/ && \
    cp /tmp/krkn-assist-fresh/fastapi_app.py /app/ && \
    cp -r /tmp/krkn-assist-fresh/utils /app/ && \
    rm -rf /tmp/krkn-assist-fresh && \
    echo "Fresh krkn-assist code updated successfully at version: $CODE_VERSION"

# Fresh clone of krkn-hub repository for scenario definitions
RUN git clone https://github.com/krkn-chaos/krkn-hub.git /tmp/krkn-hub-fresh && \
    echo "Fresh krkn-hub scenarios available for indexing at version: $CODE_VERSION" && \
    rm -rf /tmp/krkn-hub-fresh

# Run the entrypoint script
ENTRYPOINT ["./entrypoint.sh"]