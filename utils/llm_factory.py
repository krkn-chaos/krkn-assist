# Generated by Claude Sonnet 4
import os
from langchain_community.llms import Ollama, LlamaCpp
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

def create_llm_backend(backend_type="ollama", **kwargs):
    """
    Factory function to create different LLM backends
    
    Args:
        backend_type: "ollama" or "llamacpp"
        **kwargs: Additional arguments for the specific backend
    
    Returns:
        LLM instance compatible with LangChain
    """
    
    if backend_type == "ollama":
        # Original Ollama backend
        return Ollama(
            model=kwargs.get("model", "llama3.1"), 
            base_url=kwargs.get("base_url", "http://127.0.0.1:11434")
        )
    
    elif backend_type == "llamacpp":
        # New llama.cpp backend for krknctl
        model_path = kwargs.get("model_path") or os.getenv("MODEL_PATH", "/app/models/Llama-3.2-1B-Instruct-Q4_K_M.gguf")
        
        # Optional callback manager for streaming
        callback_manager = kwargs.get("callback_manager")
        if callback_manager is None and kwargs.get("streaming", False):
            callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
        
        return LlamaCpp(
            model_path=model_path,
            n_ctx=kwargs.get("n_ctx", 4096),
            n_batch=kwargs.get("n_batch", 512),
            n_threads=kwargs.get("n_threads"),
            n_gpu_layers=kwargs.get("n_gpu_layers", -1 if os.getenv("USE_GPU", "true").lower() == "true" else 0),
            temperature=kwargs.get("temperature", 0.7),
            max_tokens=kwargs.get("max_tokens", 512),
            top_p=kwargs.get("top_p", 0.95),
            verbose=kwargs.get("verbose", False),
            callback_manager=callback_manager
        )
    
    else:
        raise ValueError(f"Unsupported backend type: {backend_type}")