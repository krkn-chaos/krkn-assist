{
  "items": [
    {
      "user_input": "What is the primary objective of the 'Pod Scenarios' feature within Krkn-hub on a Kubernetes/OpenShift cluster?",
      "generated_answer": "The primary objective of the 'Pod Scenarios' feature within Krkn-hub on a Kubernetes/OpenShift cluster is to allow users to disrupt or kill random pods in a targeted namespace or across multiple namespaces, with options for configuring the number of pods to disrupt, timeout for removing pods, and expected recovery time. The feature provides a way to test the reliability of a Kubernetes/OpenShift cluster by injecting chaos into the system in a controlled manner.",
      "retrieved_context": [
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:"
      ],
      "reference_answer": "The primary objective of the 'Pod Scenarios' feature in Krkn-hub is to disrupt pods that match a specified label within a designated namespace on either a Kubernetes or OpenShift cluster, simulating chaos conditions."
    },
    {
      "user_input": "How can Cerberus be integrated with Krkn-hub's pod scenarios to monitor the cluster and determine the success or failure of a chaos injection?",
      "generated_answer": "To integrate Cerberus with Krkn-hub's pod scenarios to monitor the cluster and determine the success or failure of a chaos injection, you can follow these steps:\n\n1. Start by setting the `CERBERUS_ENABLED` environment variable to true before injecting the chaos. This will enable Cerberus to monitor the cluster.\n2. Use Krkn-hub's `podman run` command to run a scenario on the cluster. You can specify the scenario file using the `--name` or `-n` option, followed by the name of the scenario file. For example: `podman run -n my-scenario my-scenario.yaml`.\n3. Once the scenario is running, Cerberus will start collecting data on the cluster's health. You can use the `cerberus` command-line tool to query the cluster's health and get the exit code of the scenario. For example: `cerberus query -n <namespace>` or `cerberus query -p <pod_name>`.\n4. After the scenario has completed, you can use Cerberus to determine if the scenario was successful or failed. You can use the `cerberus` command-line tool to get the exit code of the scenario and check if it matches the expected outcome. For example: `cerberus query -n <namespace> -p <pod_name>` to get the exit code of the pods in the specified namespace, or `cerberus query -p <pod_name>` to get the exit code of a single pod.\n\nBy following these steps, you can integrate Cerberus with Krkn-hub's pod scenarios to monitor the cluster and determine the success or failure of a chaos injection.",
      "retrieved_context": [
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:"
      ],
      "reference_answer": "To integrate Cerberus, it must be started before injecting chaos. For the chaos injection container to auto-connect with Cerberus and enable monitoring for pass/fail evaluation, the `CERBERUS_ENABLED` environment variable must be set."
    },
    {
      "user_input": "What is the standard Podman command structure for initiating a pod disruption scenario with Krkn-hub, including how to provide the kube config and enable host environment variables?",
      "generated_answer": "To initiate a pod disruption scenario using Krkn-hub, you can use the following command structure in Podman:\n```\npodman run --name=<container_name> --net=host --env-host=true -v <path-to-kube-config>:Z $ krkn-hub pod-scenarios -d containers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n```\nThis command will run a container with the specified name and network settings, and enable the `host` environment variable. It will then run the `krkn-hub` command to initiate the pod disruption scenario.\n\nYou can also use the `--help` flag to see all available scenario options:\n```\npodman run --name=<container_name> --net=host --env-host=true -v <path-to-kube-config>:Z $ krkn-hub pod-scenarios --help\n```\nNote that the `--env-host` option is not available when using the remote Podman client (including Mac and Windows machines excluding WSL2). You will need to set each environment variable on the command line like `-e <VARIABLE>=<value>`.",
      "retrieved_context": [
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "Outputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy  \nNote  \u2013env-host:  This  option  is  not  available  with  the  remote  Podman  client,  including  Mac  and  \nWindows\n \n(excluding\n \nWSL2)\n \nmachines.\n \nWithout\n \nthe\n \n\u2013env-host\n \noption\n \nyou\u2019ll\n \nhave\n \nto\n \nset\n \neach\n \nenviornment\n \nvariable\n \non\n \nthe\n \npodman\n \ncommand\n \nline\n \nlike\n \n-e\n \n<VARIABLE>=<value>\n \n$  docker  run  $(./get_docker_params.sh)  --name=<container_name>  --net=host  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n OR   $  docker  run  -e  <VARIABLE>=<value>  --name=<container_name>  --net=host  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n  $  docker  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  docker  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario"
      ],
      "reference_answer": "The standard Podman command is `podman run --name=<container_name> --net=host --env-host=true -v <path-to-kube-config>:/home/krkn/.kube/config:Z -d containers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios`. This command runs the scenario container, mounts the kube config, and uses `--env-host=true` to allow the container to access host environment variables."
    },
    {
      "user_input": "After a Krkn-hub pod scenario has been initiated, what commands can be used with both Podman and Docker to monitor its ongoing logs and to retrieve its final exit code for pass/fail determination?",
      "generated_answer": "To monitor the logs of a Krkn-hub pod scenario after it has been initiated, you can use the `docker logs` command with the `-f` option to follow the logs stream. The output will show the exit code of the container, which can be considered as pass or fail for the scenario.\n\nHere are some examples of commands that can be used to monitor the logs of a Krkn-hub pod scenario:\n```\ndocker logs -f <container_name or container_id> # Streams Kraken logs\ndocker inspect <container-name or container-id> --format \"{{.State.ExitCode}}\" # Outputs exit code\n```\nAlternatively, you can use `podman logs` command to monitor the logs of a Krkn-hub pod scenario. The output will also show the exit code of the container, which can be considered as pass or fail for the scenario.\n```\npodman logs -f <container_name or container_id> # Streams Kraken logs\npodman inspect <container-name or container-id> --format \"{{.State.ExitCode}}\" # Outputs exit code\n```\nIt's important to note that the `docker` and `podman` commands have different options and behaviors, so it's best to consult the documentation for each command to understand how to use them properly.",
      "retrieved_context": [
        "Outputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy  \nNote  \u2013env-host:  This  option  is  not  available  with  the  remote  Podman  client,  including  Mac  and  \nWindows\n \n(excluding\n \nWSL2)\n \nmachines.\n \nWithout\n \nthe\n \n\u2013env-host\n \noption\n \nyou\u2019ll\n \nhave\n \nto\n \nset\n \neach\n \nenviornment\n \nvariable\n \non\n \nthe\n \npodman\n \ncommand\n \nline\n \nlike\n \n-e\n \n<VARIABLE>=<value>\n \n$  docker  run  $(./get_docker_params.sh)  --name=<container_name>  --net=host  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n OR   $  docker  run  -e  <VARIABLE>=<value>  --name=<container_name>  --net=host  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n  $  docker  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  docker  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "$  docker  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  docker  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy"
      ],
      "reference_answer": "To monitor ongoing logs, use `podman logs -f <container_name or container_id>` or `docker logs -f <container_name or container_id>`. To determine the final outcome (pass/fail), the exit code can be retrieved using `podman inspect <container-name or container_id> --format \"{{.State.ExitCode}}\"` or `docker inspect <container-name or container_id> --format \"{{.State.ExitCode}}\"`."
    },
    {
      "user_input": "What is a significant limitation of the `--env-host` option when running Krkn-hub scenarios with Podman on certain client types, and how should environment variables be managed in such cases?",
      "generated_answer": "The significant limitation of the `--env-host` option when running Krkn-hub scenarios with Podman on certain client types, such as Mac and Windows machines (excluding WSL2), is that it is not available. This option allows setting environment variables on the host machine before running the container, which can be useful for tweaking scenario/faults being injected. However, without `--env-host`, you'll have to set each environment variable on the Podman command line like `-e <VARIABLE>=<value>`.",
      "retrieved_context": [
        "Outputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy  \nNote  \u2013env-host:  This  option  is  not  available  with  the  remote  Podman  client,  including  Mac  and  \nWindows\n \n(excluding\n \nWSL2)\n \nmachines.\n \nWithout\n \nthe\n \n\u2013env-host\n \noption\n \nyou\u2019ll\n \nhave\n \nto\n \nset\n \neach\n \nenviornment\n \nvariable\n \non\n \nthe\n \npodman\n \ncommand\n \nline\n \nlike\n \n-e\n \n<VARIABLE>=<value>\n \n$  docker  run  $(./get_docker_params.sh)  --name=<container_name>  --net=host  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n OR   $  docker  run  -e  <VARIABLE>=<value>  --name=<container_name>  --net=host  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n  $  docker  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  docker  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #",
        "the\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:  \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-custom-metrics-profile>:/home/krkn/kraken/config/metrics-aggregated.yaml\n \n-v\n \n<path-to-custom-alerts-profile>:/home/krkn/kraken/config/alerts\n \n-v\n \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:container-scenarios\n \nCopy",
        "None \n None \nTip  Because  the  container  runs  with  a  non-root  user,  ensure  the  kube  config  is  globally  readable  \nbefore\n \nmounting\n \nit\n \nin\n \nthe\n \ncontainer.\n \nYou\n \ncan\n \nachieve\n \nthis\n \nwith\n \nthe\n \nfollowing\n \ncommands:\n \nkubectl\n \nconfig\n \nview\n \n--flatten\n \n>\n \n~/kubeconfig\n \n&&\n \nchmod\n \n444\n \n~/kubeconfig\n \n&&\n \ndocker\n \nrun\n \n$(./get_docker_params.sh)\n \n--name=<container_name>\n \n--net=host\n \n-v\n \n~kubeconfig:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:<scenario>\n \nSupported  parameters  The  following  environment  variables  can  be  set  on  the  host  running  the  container  to  tweak  the  \nscenario/faults\n \nbeing\n \ninjected:\n Example  if  \u2013env-host  is  used:  \nexport  <parameter_name>=<value>  \nOR  on  the  command  line  like  example:  \n-e  <VARIABLE>=<value>",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help"
      ],
      "reference_answer": "The `--env-host` option is not available with remote Podman clients, including those on Mac and Windows (excluding WSL2). In these situations, environment variables must be set individually on the Podman command line using the `-e <VARIABLE>=<value>` syntax for each variable."
    },
    {
      "user_input": "Why is it crucial to adjust the permissions of the kube config file before mounting it into the Krkn-hub container, and what specific commands are recommended to achieve this?",
      "generated_answer": "The question is asking why it's crucial to adjust the permissions of the kube config file before mounting it into a Krkn-hub container, and what specific commands are recommended to achieve this.\n\nTo answer this question, I can provide three sentences that explain the reasoning behind this recommendation:\n\n1. The container runs with a non-root user, so ensuring the kube config is globally readable before mounting it is important to allow the container to access the configuration file.\n2. Without proper permissions, the container may not be able to read or modify the configuration file, leading to unexpected behavior or failure to run the desired scenarios.\n3. By using the `chmod` command to set the permissions of the kube config file to 444, we ensure that the container can access and modify the configuration as needed.\n\nIn terms of specific commands, the question mentions using `kubectl config view --flatten > ~/kubeconfig` to view the configuration file, followed by `chmod 444 ~/kubeconfig` to set the permissions. The question also mentions using `docker run $(./get_docker_params.sh)` to run the container, and `-e <VARIABLE>=<value>` to set environment variables on the command line.",
      "retrieved_context": [
        "None \n None \nTip  Because  the  container  runs  with  a  non-root  user,  ensure  the  kube  config  is  globally  readable  \nbefore\n \nmounting\n \nit\n \nin\n \nthe\n \ncontainer.\n \nYou\n \ncan\n \nachieve\n \nthis\n \nwith\n \nthe\n \nfollowing\n \ncommands:\n \nkubectl\n \nconfig\n \nview\n \n--flatten\n \n>\n \n~/kubeconfig\n \n&&\n \nchmod\n \n444\n \n~/kubeconfig\n \n&&\n \ndocker\n \nrun\n \n$(./get_docker_params.sh)\n \n--name=<container_name>\n \n--net=host\n \n-v\n \n~kubeconfig:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:<scenario>\n \nSupported  parameters  The  following  environment  variables  can  be  set  on  the  host  running  the  container  to  tweak  the  \nscenario/faults\n \nbeing\n \ninjected:\n Example  if  \u2013env-host  is  used:  \nexport  <parameter_name>=<value>  \nOR  on  the  command  line  like  example:  \n-e  <VARIABLE>=<value>",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "the\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:  \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-custom-metrics-profile>:/home/krkn/kraken/config/metrics-aggregated.yaml\n \n-v\n \n<path-to-custom-alerts-profile>:/home/krkn/kraken/config/alerts\n \n-v\n \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:container-scenarios\n \nCopy",
        "Outputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy  \nNote  \u2013env-host:  This  option  is  not  available  with  the  remote  Podman  client,  including  Mac  and  \nWindows\n \n(excluding\n \nWSL2)\n \nmachines.\n \nWithout\n \nthe\n \n\u2013env-host\n \noption\n \nyou\u2019ll\n \nhave\n \nto\n \nset\n \neach\n \nenviornment\n \nvariable\n \non\n \nthe\n \npodman\n \ncommand\n \nline\n \nlike\n \n-e\n \n<VARIABLE>=<value>\n \n$  docker  run  $(./get_docker_params.sh)  --name=<container_name>  --net=host  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n OR   $  docker  run  -e  <VARIABLE>=<value>  --name=<container_name>  --net=host  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n  $  docker  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  docker  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #"
      ],
      "reference_answer": "It is crucial because the Krkn-hub container runs with a non-root user, requiring the kube config to be globally readable. This can be achieved by first flattening the config and saving it (`kubectl config view --flatten > ~/kubeconfig`), and then changing its permissions to `444` (`chmod 444 ~/kubeconfig`) before mounting."
    },
    {
      "user_input": "How can environment variables be passed to a Krkn-hub scenario container to customize its behavior, detailing both the host-based and command-line methods?",
      "generated_answer": "To pass environment variables to a Krkn-hub scenario container for customizing its behavior, you can use the `--env` option followed by the name of the environment variable and its value. For example:\n```\npodman run --name=<container_name> --net=host --env=FOO=bar <path-to-kube-config>:/home/krkn/.kube/config:Z -d containers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n```\nThis will set the `FOO` environment variable to `bar` within the container. You can also use the `-e` option on the command line to set environment variables, for example:\n```\npodman run -e FOO=bar <path-to-kube-config>:/home/krkn/.kube/config:Z -d containers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n```\nAdditionally, you can set any global variable listed in the output of the `krknctl run pod-scenarios --help` command using the `chmod` command to make them readable and writable by the container.\n\nIt's also possible to use the `KRKN_ENABLED` environment variable to enable Cerberus monitoring of the cluster and pass/fail the scenario post-chaos, refer to the documentation for more information.",
      "retrieved_context": [
        "Outputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy  \nNote  \u2013env-host:  This  option  is  not  available  with  the  remote  Podman  client,  including  Mac  and  \nWindows\n \n(excluding\n \nWSL2)\n \nmachines.\n \nWithout\n \nthe\n \n\u2013env-host\n \noption\n \nyou\u2019ll\n \nhave\n \nto\n \nset\n \neach\n \nenviornment\n \nvariable\n \non\n \nthe\n \npodman\n \ncommand\n \nline\n \nlike\n \n-e\n \n<VARIABLE>=<value>\n \n$  docker  run  $(./get_docker_params.sh)  --name=<container_name>  --net=host  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n OR   $  docker  run  -e  <VARIABLE>=<value>  --name=<container_name>  --net=host  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n  $  docker  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  docker  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #",
        "None \n None \nTip  Because  the  container  runs  with  a  non-root  user,  ensure  the  kube  config  is  globally  readable  \nbefore\n \nmounting\n \nit\n \nin\n \nthe\n \ncontainer.\n \nYou\n \ncan\n \nachieve\n \nthis\n \nwith\n \nthe\n \nfollowing\n \ncommands:\n \nkubectl\n \nconfig\n \nview\n \n--flatten\n \n>\n \n~/kubeconfig\n \n&&\n \nchmod\n \n444\n \n~/kubeconfig\n \n&&\n \ndocker\n \nrun\n \n$(./get_docker_params.sh)\n \n--name=<container_name>\n \n--net=host\n \n-v\n \n~kubeconfig:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:<scenario>\n \nSupported  parameters  The  following  environment  variables  can  be  set  on  the  host  running  the  container  to  tweak  the  \nscenario/faults\n \nbeing\n \ninjected:\n Example  if  \u2013env-host  is  used:  \nexport  <parameter_name>=<value>  \nOR  on  the  command  line  like  example:  \n-e  <VARIABLE>=<value>",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy"
      ],
      "reference_answer": "Environment variables can be passed by setting them on the host running the container using `export <parameter_name>=<value>` if the `--env-host` option is used. Alternatively, they can always be passed directly on the command line when running the container using the `-e <VARIABLE>=<value>` flag."
    },
    {
      "user_input": "What is the function of the `NAMESPACE` parameter in Krkn-hub pod scenarios, and what is its default value, including its support for advanced matching?",
      "generated_answer": "The `NAMESPACE` parameter in Krkn-hub pod scenarios is used to target a specific namespace in the Kubernetes cluster. The default value of this parameter is \"openshift-*\" which means that the scenario will run on any namespace that matches the regular expression \"openshift-*\".\n\nThe `NAMESPACE` parameter is used in conjunction with the `POD_LABEL` parameter to target specific pods within a namespace. If `POD_LABEL` is not specified, then the `NAMESPACE` parameter will be used to match all pods in the targeted namespace.\n\nThe `NAME_PATTERN` parameter is a regex pattern that is used to match the pods in the targeted namespace when `POD_LABEL` is not specified. The default value of this parameter is \".*\" which means that any pod in the targeted namespace will be matched.\n\nIn summary, the `NAMESPACE` parameter in Krkn-hub pod scenarios allows you to target a specific namespace in the Kubernetes cluster and can be used in conjunction with the `POD_LABEL` parameter to target specific pods within that namespace.",
      "retrieved_context": [
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "scenario/faults\n \nbeing\n \ninjected:\n Example  if  \u2013env-host  is  used:  \nexport  <parameter_name>=<value>  \nOR  on  the  command  line  like  example:  \n-e  <VARIABLE>=<value>   \nSee  list  of  variables  that  apply  to  all  scenarios  here that  can  be  used/set  in  addition  to  these  scenario  specific  variables  Parameter  Description  Default  \nNAMESPACE  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nopenshift-.*  \nPOD_LABEL  Label  of  the  pod(s)  to  target  \n\"\"  \nNAME_PATTERN  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \n.*",
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario"
      ],
      "reference_answer": "The `NAMESPACE` parameter specifies the targeted namespace(s) within the cluster where the pod disruption will occur. It supports regular expressions for flexible matching, and its default value is `openshift-.*`, which targets all OpenShift system namespaces."
    },
    {
      "user_input": "Explain the interplay between the `POD_LABEL` and `NAME_PATTERN` parameters in determining which pods are targeted for disruption, and what their default behaviors are.",
      "generated_answer": "The `POD_LABEL` and `NAME_PATTERN` parameters work together to determine which pods are targeted for disruption in a Krkn-hub scenario. The `POD_LABEL` parameter specifies the label of the pod(s) to target, while the `NAME_PATTERN` parameter defines a regular expression pattern to match the pods in the target namespace when the `POD_LABEL` is not specified.\n\nWhen both parameters are provided, Krkn-hub will disrupt the pods that match the label in the target namespace and also match the pattern defined in the `NAME_PATTERN` parameter. If only the `POD_LABEL` parameter is provided, Krkn-hub will disrupt the pods that match the specified label in the target namespace.\n\nThe `DISRUPTION_COUNT`, `KILL_TIMEOUT`, and `EXPECTED_RECOVERY_TIME` parameters determine the number of pods to disrupt, the timeout for waiting for the target pods to be removed, and the expected recovery time, respectively. These parameters can be used to customize the behavior of the scenario.\n\nIn summary, the `POD_LABEL` and `NAME_PATTERN` parameters work together to determine which pods are targeted for disruption in a Krkn-hub scenario, while the other parameters can be used to customize the scenario's behavior.",
      "retrieved_context": [
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "scenario/faults\n \nbeing\n \ninjected:\n Example  if  \u2013env-host  is  used:  \nexport  <parameter_name>=<value>  \nOR  on  the  command  line  like  example:  \n-e  <VARIABLE>=<value>   \nSee  list  of  variables  that  apply  to  all  scenarios  here that  can  be  used/set  in  addition  to  these  scenario  specific  variables  Parameter  Description  Default  \nNAMESPACE  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nopenshift-.*  \nPOD_LABEL  Label  of  the  pod(s)  to  target  \n\"\"  \nNAME_PATTERN  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \n.*",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy"
      ],
      "reference_answer": "The `POD_LABEL` parameter specifies the label of the pod(s) to target, with a default of an empty string. If `POD_LABEL` is not specified, the `NAME_PATTERN` parameter is used instead to match pods within the `NAMESPACE` using a regex pattern. The `NAME_PATTERN` defaults to `.*`, meaning it will match all pods in the targeted namespace if no specific label is provided."
    },
    {
      "user_input": "What do the `DISRUPTION_COUNT`, `KILL_TIMEOUT`, and `EXPECTED_RECOVERY_TIME` parameters control in a Krkn-hub pod scenario, and what are their default values?",
      "generated_answer": "The `DISRUPTION_COUNT`, `KILL_TIMEOUT`, and `EXPECTED_RECOVERY_TIME` parameters control various aspects of disrupting pods in a Krkn-hub scenario. Here are the default values for each parameter:\n\n* `DISRUPTION_COUNT`: 1 (meaning one pod will be disrupted)\n* `KILL_TIMEOUT`: 180 seconds (meaning the target pods will be removed after waiting for this amount of time)\n* `EXPECTED_RECOVERY_TIME`: 120 seconds (meaning the pods are expected to recover within this amount of time)\n\nNote that these values can be modified through scenario configuration files.",
      "retrieved_context": [
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:",
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy"
      ],
      "reference_answer": "The `DISRUPTION_COUNT` parameter specifies the number of pods to disrupt, defaulting to `1`. The `KILL_TIMEOUT` parameter sets the maximum time (in seconds) to wait for target pods to be removed, defaulting to `180`. The `EXPECTED_RECOVERY_TIME` parameter defines the timeout (in seconds) for disrupted pods to recover, defaulting to `120`, and the scenario fails if recovery doesn't occur within this period."
    },
    {
      "user_input": "Beyond specific namespace targeting, how can the `NAMESPACE` environment variable be configured to randomly disrupt pods in OpenShift system namespaces, and what additional mode can be enabled for continuous reliability testing?",
      "generated_answer": "To randomly disrupt pods in OpenShift system namespaces using the `NAMESPACE` environment variable, you can set it to `openshift-.*` and use the `krknctl` command with the `--disruption-count`, `--kill-timeout`, and `--expected-recovery-time` parameters.\n\nFor example, you can run the following command:\n```\nkrknctl run pod-scenarios --namespace openshift-* --disruption-count 1 --kill-timeout 180 --expected-recovery-time 120\n```\nThis will disrupt 1 pod in each namespace in the `openshift-*` namespace, wait for 180 seconds for the pods to be removed, and fail the scenario if the pods do not recover within 120 seconds.\n\nYou can also enable the `DAEMON_MODE` for continuous reliability testing, which will disrupt the pods every X seconds in the background to check the reliability.\n\nNote that when using custom metrics profiles or alerts profiles with `CAPTURE_METRICS` or `ENABLE_ALERTS` enabled, you can mount the metrics profile from the host on which the container is running and use the `/home/krkn/kraken/config/metrics-aggregated.yaml` file to configure the metrics aggregation.\n\nAdditionally, you can use the `krknctl` command with the `--help` parameter to see all available scenario options.",
      "retrieved_context": [
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:",
        "scenario/faults\n \nbeing\n \ninjected:\n Example  if  \u2013env-host  is  used:  \nexport  <parameter_name>=<value>  \nOR  on  the  command  line  like  example:  \n-e  <VARIABLE>=<value>   \nSee  list  of  variables  that  apply  to  all  scenarios  here that  can  be  used/set  in  addition  to  these  scenario  specific  variables  Parameter  Description  Default  \nNAMESPACE  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nopenshift-.*  \nPOD_LABEL  Label  of  the  pod(s)  to  target  \n\"\"  \nNAME_PATTERN  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \n.*",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy"
      ],
      "reference_answer": "The `NAMESPACE` environment variable can be set to `openshift-.*` to randomly select and disrupt pods within OpenShift system namespaces. For continuous reliability testing, `DAEMON_MODE` can be enabled to disrupt pods every 'x' seconds in the background."
    },
    {
      "user_input": "When `CAPTURE_METRICS` or `ENABLE_ALERTS` are active, how should custom metrics and alerts profiles be provided to the Krkn-hub container, including the specific internal paths for mounting?",
      "generated_answer": "To provide custom metrics and alerts profiles for the Krkn-hub container, you can mount the profiles from the host machine using the `-v` option. For example:\n```\n$ podman run --name=<container_name> --net=host --env-host=true -v <path-to-custom-metrics-profile>:/home/krkn/kraken/config/metrics-aggregated.yaml -d containers.krkn-chaos.dev/krkn-chaos/krkn-hub:container-scenarios\n```\nYou can also mount the alerts profile using the same option:\n```\n$ podman run --name=<container_name> --net=host --env-host=true -v <path-to-custom-alerts-profile>:/home/krkn/kraken/config/alerts -d containers.krkn-chaos.dev/krkn-chaos/krkn-hub:container-scenarios\n```\nNote that you need to set the `NAMESPACE` environment variable to `openshift-*` to pick and disrupt pods randomly in OpenShift namespaces. Also, you can enable the `DAEMON_MODE` to disrupt the pods every x seconds in the background to check the reliability.\n\nIn case you are using a custom metrics profile or alerts profile when `CAPTURE_METRICS` or `ENABLE_ALERTS` is enabled, you can mount the profiles from the host machine using the `-v` option. For example:\n```\n$ podman run --name=<container_name> --net=host --env-host=true -v ~/metrics-aggregated.yaml:/home/krkn/kraken/config/metrics-aggregated.yaml -d containers.krkn-chaos.dev/krkn-chaos/krkn-hub:container-scenarios\n```\n```\n$ podman run --name=<container_name> --net=host --env-host=true -v ~/alerts:/home/krkn/kraken/config/alerts -d containers.krkn-chaos.dev/krkn-chaos/krkn-hub:container-scenarios\n```\nNote that you can also use the `kubectl config view --flatten` command to view the Kube config and then mount it in the container using the `-v` option. Additionally, you can set the `KUBECONFIG` environment variable to the path of the Kube config file before mounting it in the container.",
      "retrieved_context": [
        "the\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:  \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-custom-metrics-profile>:/home/krkn/kraken/config/metrics-aggregated.yaml\n \n-v\n \n<path-to-custom-alerts-profile>:/home/krkn/kraken/config/alerts\n \n-v\n \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:container-scenarios\n \nCopy",
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:",
        "None \n None \nTip  Because  the  container  runs  with  a  non-root  user,  ensure  the  kube  config  is  globally  readable  \nbefore\n \nmounting\n \nit\n \nin\n \nthe\n \ncontainer.\n \nYou\n \ncan\n \nachieve\n \nthis\n \nwith\n \nthe\n \nfollowing\n \ncommands:\n \nkubectl\n \nconfig\n \nview\n \n--flatten\n \n>\n \n~/kubeconfig\n \n&&\n \nchmod\n \n444\n \n~/kubeconfig\n \n&&\n \ndocker\n \nrun\n \n$(./get_docker_params.sh)\n \n--name=<container_name>\n \n--net=host\n \n-v\n \n~kubeconfig:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:<scenario>\n \nSupported  parameters  The  following  environment  variables  can  be  set  on  the  host  running  the  container  to  tweak  the  \nscenario/faults\n \nbeing\n \ninjected:\n Example  if  \u2013env-host  is  used:  \nexport  <parameter_name>=<value>  \nOR  on  the  command  line  like  example:  \n-e  <VARIABLE>=<value>",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy"
      ],
      "reference_answer": "When `CAPTURE_METRICS` or `ENABLE_ALERTS` are active, custom profiles must be mounted from the host into the container using volume mounts. The custom metrics profile should be mounted to `/home/krkn/kraken/config/metrics-aggregated.yaml`, and the custom alerts profile to `/home/krkn/kraken/config/alerts`."
    },
    {
      "user_input": "When executing a `pod-scenarios` run with `krknctl`, what are the different mechanisms available for precisely identifying the target pods for disruption, and how do they interact?",
      "generated_answer": "There are several mechanisms available for precisely identifying the target pods for disruption when executing a `pod-scenarios` run with `krknctl`. These include:\n\n1. Namespace-based disruption: This mechanism allows you to target pods in a specific namespace by specifying the `--namespace` parameter. For example, you can use `--namespace openshift-*` to target all pods in the `openshift-*` namespace.\n2. Label-based disruption: This mechanism allows you to target pods based on a label selector. You can specify the `--label-selector` parameter to match pods with a specific label. For example, you can use `--label-selector k8s-app=kube-scheduler` to target all pods labeled with `k8s-app=kube-scheduler`.\n3. Name pattern-based disruption: This mechanism allows you to target pods based on a name pattern. You can specify the `--name-pattern` parameter to match pods with a specific name pattern. For example, you can use `--name-pattern .*kubernetes*` to target all pods starting with `kubernetes`.\n4. Disruption count: This mechanism allows you to specify the number of pods to disrupt. You can use the `--disruption-count` parameter to specify the number of pods to disrupt. For example, you can use `--disruption-count 3` to disrupt 3 pods.\n5. Kill timeout: This mechanism allows you to specify the timeout for waiting for the target pods to be removed after disruption. You can use the `--kill-timeout` parameter to specify the timeout in seconds. For example, you can use `--kill-timeout 180` to wait for 180 seconds before considering a scenario pass/fail.\n6. Expected recovery time: This mechanism allows you to specify the expected recovery time for the disrupted pods. You can use the `--expected-recovery-time` parameter to specify the expected recovery time in seconds. For example, you can use `--expected-recovery-time 120` to expect the disrupted pods to recover within 120 seconds.\n\nThese mechanisms can be combined and used together to precisely identify the target pods for disruption. The `krknctl` command will use these mechanisms to execute the scenario and return the result.",
      "retrieved_context": [
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:"
      ],
      "reference_answer": "To precisely identify target pods for disruption, users can leverage three parameters: `--namespace`, `--pod-label`, and `--name-pattern`. The `--namespace` parameter allows targeting pods within a specific namespace, supporting regular expressions, with a default of `openshift-*`. The `--pod-label` parameter targets pods based on a specific label, such as \"app=test\". If `--pod-label` is not specified, the `--name-pattern` parameter, which defaults to `.*`, is used as a regex pattern to match pods within the specified `--namespace`."
    },
    {
      "user_input": "Describe the two critical timeout parameters in `krknctl run pod-scenarios` and explain their distinct roles in determining the scenario's outcome.",
      "generated_answer": "The two critical timeout parameters in `krknctl run pod-scenarios` are `KILL_TIMEOUT` and `EXPECTED_RECOVERY_TIME`. These parameters determine the scenario's outcome by setting the maximum time allowed for a pod to be disrupted or recovered, respectively.\n\n`KILL_TIMEOUT` specifies the timeout in seconds for waiting for the target pods to be removed after disruption. If the pods are not removed within this time frame, the scenario fails. This parameter is important because it ensures that the scenario does not hang indefinitely if there are issues with the disruption process.\n\n`EXPECTED_RECOVERY_TIME` specifies the maximum time allowed for a pod to recover after disruption. If the pod does not recover within this time frame, the scenario fails. This parameter is important because it ensures that the scenario accounts for potential delays in pod recovery and does not expect immediate recovery.\n\nIn summary, `KILL_TIMEOUT` determines the maximum time allowed for disrupting a pod, while `EXPECTED_RECOVERY_TIME` determines the maximum time allowed for a pod to recover after disruption. Both parameters play critical roles in determining the scenario's outcome and ensuring that the scenario runs successfully.",
      "retrieved_context": [
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy"
      ],
      "reference_answer": "The two critical timeout parameters are `--kill-timeout` and `--expected-recovery-time`. The `--kill-timeout` parameter, defaulting to 180 seconds, specifies the maximum time to wait for the target pod(s) to be removed from the cluster. In contrast, the `--expected-recovery-time` parameter, defaulting to 120 seconds, dictates the maximum time allowed for a disrupted pod to recover. If a pod fails to recover within this `expected-recovery-time`, the entire scenario will be marked as a failure."
    },
    {
      "user_input": "If a user wants to disrupt more than one pod in a `pod-scenarios` run, which parameter needs to be adjusted, and what is the default behavior if this parameter is not explicitly set?",
      "generated_answer": "To disrupt more than one pod in a `pod-scenarios` run, you can set the `--disruption-count` parameter. The default behavior if this parameter is not explicitly set is to disrupt only one pod.\n\nFor example, you can use the following command to disrupt multiple pods:\n```bash\nkrknctl run pod-scenarios --disruption-count=2\n```\nThis will disrupt two pods in the targeted namespace. You can adjust the value of `--disruption-count` to disrupt a different number of pods.",
      "retrieved_context": [
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "scenario/faults\n \nbeing\n \ninjected:\n Example  if  \u2013env-host  is  used:  \nexport  <parameter_name>=<value>  \nOR  on  the  command  line  like  example:  \n-e  <VARIABLE>=<value>   \nSee  list  of  variables  that  apply  to  all  scenarios  here that  can  be  used/set  in  addition  to  these  scenario  specific  variables  Parameter  Description  Default  \nNAMESPACE  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nopenshift-.*  \nPOD_LABEL  Label  of  the  pod(s)  to  target  \n\"\"  \nNAME_PATTERN  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \n.*",
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Why  pod  scenarios  are  important:   \n \nUse  Cases  and  Importance  of  Pod-Level  Chaos  Scenarios  \n1.  Single  Pod  Deletion  \u25cf  Use  Case:  Simulates  unplanned  deletion  of  a  single  pod  \u25cf  Why  It's  Important:  Validates  whether  the  ReplicaSet  or  Deployment  automatically  \ncreates\n \na\n \nreplacement.\n \u25cf  Customer  Impact:  Ensures  continuous  service  even  if  a  pod  unexpectedly  crashes.  \u25cf  Recovery  Timing:  Typically  <10  seconds  for  stateless  apps  (seen  in  Krkn  telemetry  \noutput).\n \u25cf  HA  Indicator:  Pod  is  automatically  rescheduled  and  becomes  Ready  without  manual  \nintervention.\n  Kubectl  delete  pod  <pod-name> delete  pod  <pod-name>  Kubectl  get  pods  #  watch  for  new  pods \n2.  Multiple  Pods  Deleted  Simultaneously  \u25cf  Use  Case:  Simulates  a  larger  failure  event,  such  as  a  node  crash  or  AZ  outage.  \u25cf  Why  It's  Important:  Tests  whether  the  system  has  enough  resources  and  policies  to  \nrecover"
      ],
      "reference_answer": "To disrupt more than one pod, the user needs to adjust the `--disruption-count` parameter. By default, if this parameter is not explicitly set, `krknctl run pod-scenarios` will only disrupt a single pod, as its default value is 1."
    },
    {
      "user_input": "What is the default scope for targeting pods in a `krknctl run pod-scenarios` execution if no specific namespace, label, or name pattern is provided by the user?",
      "generated_answer": "If no specific namespace, label, or name pattern is provided by the user when running `krknctl run pod-scenarios`, then the default scope for targeting pods is the entire cluster, as specified in the context document. This means that the command will target all pods in the cluster that match the label selector specified in the scenario configuration file.",
      "retrieved_context": [
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "scenario/faults\n \nbeing\n \ninjected:\n Example  if  \u2013env-host  is  used:  \nexport  <parameter_name>=<value>  \nOR  on  the  command  line  like  example:  \n-e  <VARIABLE>=<value>   \nSee  list  of  variables  that  apply  to  all  scenarios  here that  can  be  used/set  in  addition  to  these  scenario  specific  variables  Parameter  Description  Default  \nNAMESPACE  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nopenshift-.*  \nPOD_LABEL  Label  of  the  pod(s)  to  target  \n\"\"  \nNAME_PATTERN  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \n.*",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario"
      ],
      "reference_answer": "If no specific targeting parameters are provided, the `krknctl run pod-scenarios` execution will default to targeting pods within any namespace matching the `openshift-*` regex pattern, as specified by the `--namespace` parameter's default. Furthermore, within these namespaces, it will match any pod name due to the `--name-pattern` defaulting to `.*` when `--pod-label` is not specified."
    },
    {
      "user_input": "How can a user obtain a comprehensive list of all configurable options and their descriptions for the `krknctl run pod-scenarios` command?",
      "generated_answer": "To obtain a comprehensive list of all configurable options and their descriptions for the `krknctl run pod-scenarios` command, you can use the `--help` flag. This will display all the available options and their descriptions in a concise format. Here is an example of what you might see:\n```\n krknctl run pod-scenarios --help\n\nUsage:  krknctl run <command> [<parameter>:]<value>\n\nwhere:\n  command      The command to execute (e.g. \"pod-scenarios\")\n  parameter    The name of the parameter to set (e.g. \"namespace\", \"label\", etc.)\n  value        The value to set the parameter to\n\nAvailable options for \"pod-scenarios\":\n\n  -h, --help                   Show this help message and exit\n  -n, --namespace <value>     Targeted namespace in the cluster (supports regex)\n  -l, --label <value>          Label of the pod(s) to target\n  -p, --name-pattern <value>    Regex pattern to match the pods in NAMESPACE when POD_LABEL is not specified\n  -k, --kill-timeout <value>   Timeout to wait for the target pod(s) to be removed in seconds\n  -e, --expected-recovery-time  Fails if the pod disrupted does not recover within the timeout set\n```\nThis will give you a comprehensive list of all the available options and their descriptions for the `krknctl run pod-scenarios` command. You can adjust the schema reference to point to the schema file in your IDE for code completion and documentation.",
      "retrieved_context": [
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "scenario/faults\n \nbeing\n \ninjected:\n Example  if  \u2013env-host  is  used:  \nexport  <parameter_name>=<value>  \nOR  on  the  command  line  like  example:  \n-e  <VARIABLE>=<value>   \nSee  list  of  variables  that  apply  to  all  scenarios  here that  can  be  used/set  in  addition  to  these  scenario  specific  variables  Parameter  Description  Default  \nNAMESPACE  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nopenshift-.*  \nPOD_LABEL  Label  of  the  pod(s)  to  target  \n\"\"  \nNAME_PATTERN  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \n.*"
      ],
      "reference_answer": "A user can obtain a comprehensive list of all available scenario options and their descriptions for the `krknctl run pod-scenarios` command by executing `krknctl run pod-scenarios --help`."
    },
    {
      "user_input": "Differentiate between the 'Single Pod Deletion' and 'Multiple Pods Deleted Simultaneously' chaos scenarios regarding their simulated failure types and the primary customer impact each aims to validate or expose.",
      "generated_answer": "The \"Single Pod Deletion\" and \"Multiple Pods Deleted Simultaneously\" chaos scenarios differ in their focus and purpose.\n\nThe \"Single Pod Deletion\" scenario is important because it validates whether the ReplicaSet or Deployment automatically creates a replacement pod when a single pod unexpectedly crashes. This test ensures continuous service even if a pod unexpectedly crashes. The recovery timing for this scenario is typically within 10 seconds for stateless apps, as seen in Kubernetes telemetry output. Additionally, this scenario provides an HA indicator, indicating whether the system can automatically reschedule and make the replaced pod ready without manual intervention.\n\nOn the other hand, the \"Multiple Pods Deleted Simultaneously\" scenario is important because it tests whether the system has enough resources and policies to recover from a larger failure event, such as a node crash or AZ outage. This test validates whether the system can handle simultaneous deletion of multiple pods and ensure their replacement within a reasonable time frame.\n\nIn summary, while both scenarios are important for testing Kubernetes' ability to handle unexpected events, they differ in their focus and purpose. The \"Single Pod Deletion\" scenario tests the system's ability to handle a single pod failure, while the \"Multiple Pods Deleted Simultaneously\" scenario tests the system's ability to handle multiple pod failures simultaneously.",
      "retrieved_context": [
        "Why  pod  scenarios  are  important:   \n \nUse  Cases  and  Importance  of  Pod-Level  Chaos  Scenarios  \n1.  Single  Pod  Deletion  \u25cf  Use  Case:  Simulates  unplanned  deletion  of  a  single  pod  \u25cf  Why  It's  Important:  Validates  whether  the  ReplicaSet  or  Deployment  automatically  \ncreates\n \na\n \nreplacement.\n \u25cf  Customer  Impact:  Ensures  continuous  service  even  if  a  pod  unexpectedly  crashes.  \u25cf  Recovery  Timing:  Typically  <10  seconds  for  stateless  apps  (seen  in  Krkn  telemetry  \noutput).\n \u25cf  HA  Indicator:  Pod  is  automatically  rescheduled  and  becomes  Ready  without  manual  \nintervention.\n  Kubectl  delete  pod  <pod-name> delete  pod  <pod-name>  Kubectl  get  pods  #  watch  for  new  pods \n2.  Multiple  Pods  Deleted  Simultaneously  \u25cf  Use  Case:  Simulates  a  larger  failure  event,  such  as  a  node  crash  or  AZ  outage.  \u25cf  Why  It's  Important:  Tests  whether  the  system  has  enough  resources  and  policies  to  \nrecover",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:",
        "config\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario \nKill  a  pod.  :heavy_check_mark:  \nEtcd Kills  a  single/multiple  etcd  replicas.  \n:heavy_check_mark:"
      ],
      "reference_answer": "The 'Single Pod Deletion' scenario simulates an unplanned, isolated failure of a single pod, primarily validating whether the ReplicaSet or Deployment automatically creates a replacement to ensure continuous service with minimal customer impact. In contrast, 'Multiple Pods Deleted Simultaneously' simulates a larger, more widespread failure event, such as a node crash or an Availability Zone outage. This scenario tests the system's ability to recover gracefully with sufficient resources and policies, as a failure of all pods belonging to a service in this scenario directly impacts user experience if not properly mitigated by distributed replicas."
    },
    {
      "user_input": "Explain the significance of 'Pod Eviction' as a chaos scenario, detailing its typical triggers and the specific configurations that are crucial for ensuring zero customer impact during such an event.",
      "generated_answer": "Pod eviction, also known as soft disruption, is a chaos scenario that involves the graceful termination and restart of a service without impacting the user experience. It is triggered by Kubernetes itself during node upgrades or scaling down. The significance of pod eviction lies in ensuring zero customer impact during such events.\n\nTo ensure this, it's important to configure readiness and liveness probes, as well as pause and rollback budgets (PDBs), which help the system automatically create replacements for deleted pods. Additionally, setting up rolling updates can help minimize the impact of the disruption on the application.\n\nTo know if a service is highly available, you can check if it has enough resources and policies to recover from failures. You can also use tools like Krknctl to run pod scenarios and test the system's ability to handle disruptions.\n\nIn summary, pod eviction is an important chaos scenario that helps ensure zero customer impact during node upgrades or scaling down. By configuring readiness and liveness probes, PDBs, and rolling updates, you can minimize the impact of any disruptions to your service.",
      "retrieved_context": [
        "Why  pod  scenarios  are  important:   \n \nUse  Cases  and  Importance  of  Pod-Level  Chaos  Scenarios  \n1.  Single  Pod  Deletion  \u25cf  Use  Case:  Simulates  unplanned  deletion  of  a  single  pod  \u25cf  Why  It's  Important:  Validates  whether  the  ReplicaSet  or  Deployment  automatically  \ncreates\n \na\n \nreplacement.\n \u25cf  Customer  Impact:  Ensures  continuous  service  even  if  a  pod  unexpectedly  crashes.  \u25cf  Recovery  Timing:  Typically  <10  seconds  for  stateless  apps  (seen  in  Krkn  telemetry  \noutput).\n \u25cf  HA  Indicator:  Pod  is  automatically  rescheduled  and  becomes  Ready  without  manual  \nintervention.\n  Kubectl  delete  pod  <pod-name> delete  pod  <pod-name>  Kubectl  get  pods  #  watch  for  new  pods \n2.  Multiple  Pods  Deleted  Simultaneously  \u25cf  Use  Case:  Simulates  a  larger  failure  event,  such  as  a  node  crash  or  AZ  outage.  \u25cf  Why  It's  Important:  Tests  whether  the  system  has  enough  resources  and  policies  to  \nrecover",
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "recover\n \ngracefully.\n \u25cf  Customer  Impact:  If  all  pods  of  a  service  fail,  user  experience  is  directly  impacted.  \u25cf  HA  Indicator:  Application  can  continue  functioning  from  other  replicas  across  \nzones/nodes.\n \n \n3.  Pod  Eviction  (Soft  Disruption)  \u25cf  Use  Case:  Triggered  by  Kubernetes  itself  during  node  upgrades  or  scaling  down.  \u25cf  Why  It's  Important:  Ensures  graceful  termination  and  restart  elsewhere  without  user  \nimpact.\n \u25cf  Customer  Impact:  Should  be  zero  if  readiness/liveness  probes  and  PDBs  are  correctly  \nconfigured.\n \u25cf  HA  Indicator:  Rolling  disruption  does  not  take  down  the  whole  application.  \n \nHow  to  Know  If  It's  Highly  Available",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help"
      ],
      "reference_answer": "Pod Eviction is a significant chaos scenario because it simulates a 'soft disruption' triggered by Kubernetes itself, typically during routine operations like node upgrades or scaling down. Its importance lies in ensuring graceful termination and restart of pods elsewhere without user impact. For zero customer impact, it is crucial that readiness and liveness probes are correctly configured to manage pod health and traffic flow, and that Pod Disruption Budgets (PDBs) are in place to define the minimum number of available replicas, thereby ensuring that a rolling disruption does not take down the entire application."
    },
    {
      "user_input": "What specific metrics and observations from Krkn telemetry are crucial for quantitatively assessing the high availability of an application after a chaos engineering run?",
      "generated_answer": "The specific metrics and observations from Krkn telemetry that are crucial for quantitatively assessing the high availability of an application after a chaos engineering run include:\n\n1. Recovery time: The time it takes for the pods to recover after being disrupted. This metric is important for evaluating the HA of the application.\n2. Pod reschedule latency: The time it takes for the Kubernetes controller to reschedule the pods after they have been disrupted. This metric provides information about the performance of the Kubernetes cluster.\n3. Service uptime: The duration that the service is available after the chaos engineering run. This metric is important for evaluating the overall availability of the application.\n4. Downtime: The duration that the service is not available after the chaos engineering run. This metric provides information about the impact of the chaos engineering run on the service availability.\n5. Automatic recovery: Whether the recovery process is automatic or requires manual intervention. This metric provides information about the level of automation of the recovery process.\n6. Service downtime distribution: The distribution of downtime for the service after the chaos engineering run. This metric provides information about the severity of the downtime and how it affects the overall availability of the application.\n7. Pod distribution across nodes/availability zones: The distribution of pods across nodes or availability zones after the chaos engineering run. This metric provides information about the level of replication and how it affects the overall availability of the application.\n8. Service uptime distribution: The distribution of service uptime after the chaos engineering run. This metric provides information about the level of availability of the service and how it affects the overall HA of the application.\n\nBy analyzing these metrics, you can evaluate the effectiveness of the chaos engineering run in terms of high availability and identify areas for improvement.",
      "retrieved_context": [
        "1.  Multiple  Replicas  Exist:  Confirmed  by  checking  kubectl  get  deploy and  seeing  >1  \nreplicas.\n 2.  Pods  Distributed  Across  Nodes/availability  zones:  Using  topologySpreadConstraints or  observing  pod  distribution  in  kubectl  get  pods  \n-o\n \nwide.  3.  Service  Uptime  Remains  Unaffected:  During  chaos  test,  verify  app  availability  (  \nsynthetic\n \nprobes,\n \nPrometheus\n \nalerts,\n \netc).\n 4.  Recovery  Is  Automatic:  No  manual  intervention  needed  to  restore  service.  5.  Krkn  Telemetry  Indicators:  End  of  run  data  includes  recovery  times,  pod  reschedule  \nlatency,\n \nand\n \nservice\n \ndowntime\n \nwhich\n \nare\n \nvital\n \nmetrics\n \nfor\n \nassessing\n \nHA.",
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help"
      ],
      "reference_answer": "Krkn telemetry provides vital end-of-run data that is crucial for quantitatively assessing an application's high availability. Key indicators include recovery times, which measure how quickly services are restored post-disruption; pod reschedule latency, indicating the time taken for new pods to become ready and operational; and service downtime, which quantifies any periods of unavailability experienced by the application. These metrics collectively offer a precise measure of the system's resilience and its ability to recover automatically."
    },
    {
      "user_input": "Beyond automatic recovery, what are the key architectural and operational indicators, as described in the text, that confirm an application is highly available in a Kubernetes environment?",
      "generated_answer": "Based on the context provided, the key architectural and operational indicators that confirm an application is highly available in a Kubernetes environment are:\n\n1. Multiple replicas exist: Confirmed by checking `kubectl get deploy` and seeing >1 replicas.\n2. Pods are distributed across nodes/availability zones: Using topologySpreadConstraints or observing pod distribution in `kubectl get pods -o wide`.\n3. Service uptime remains unaffected during chaos tests, synthetic probes, Prometheus alerts, etc.\n4. Recovery is automatic: No manual intervention needed to restore service.\n5. KRKN telemetry indicators: End of run data includes recovery times, pod reschedule latency, and service downtime, which are vital metrics for assessing HA.\n6. Customer impact: If all pods of a service fail, user experience is directly impacted.\n7. HA indicator: Application can continue functioning from other replicas across zones/nodes.\n8. Pod eviction (soft disruption): Ensures graceful termination and restart elsewhere without user impact.\n\nBy evaluating these indicators, you can determine if an application is highly available in a Kubernetes environment.",
      "retrieved_context": [
        "1.  Multiple  Replicas  Exist:  Confirmed  by  checking  kubectl  get  deploy and  seeing  >1  \nreplicas.\n 2.  Pods  Distributed  Across  Nodes/availability  zones:  Using  topologySpreadConstraints or  observing  pod  distribution  in  kubectl  get  pods  \n-o\n \nwide.  3.  Service  Uptime  Remains  Unaffected:  During  chaos  test,  verify  app  availability  (  \nsynthetic\n \nprobes,\n \nPrometheus\n \nalerts,\n \netc).\n 4.  Recovery  Is  Automatic:  No  manual  intervention  needed  to  restore  service.  5.  Krkn  Telemetry  Indicators:  End  of  run  data  includes  recovery  times,  pod  reschedule  \nlatency,\n \nand\n \nservice\n \ndowntime\n \nwhich\n \nare\n \nvital\n \nmetrics\n \nfor\n \nassessing\n \nHA.",
        "recover\n \ngracefully.\n \u25cf  Customer  Impact:  If  all  pods  of  a  service  fail,  user  experience  is  directly  impacted.  \u25cf  HA  Indicator:  Application  can  continue  functioning  from  other  replicas  across  \nzones/nodes.\n \n \n3.  Pod  Eviction  (Soft  Disruption)  \u25cf  Use  Case:  Triggered  by  Kubernetes  itself  during  node  upgrades  or  scaling  down.  \u25cf  Why  It's  Important:  Ensures  graceful  termination  and  restart  elsewhere  without  user  \nimpact.\n \u25cf  Customer  Impact:  Should  be  zero  if  readiness/liveness  probes  and  PDBs  are  correctly  \nconfigured.\n \u25cf  HA  Indicator:  Rolling  disruption  does  not  take  down  the  whole  application.  \n \nHow  to  Know  If  It's  Highly  Available",
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:",
        "config\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario \nKill  a  pod.  :heavy_check_mark:  \nEtcd Kills  a  single/multiple  etcd  replicas.  \n:heavy_check_mark:"
      ],
      "reference_answer": "To confirm an application's high availability in a Kubernetes environment, several architectural and operational indicators are crucial. Architecturally, it's essential to verify that multiple replicas exist for deployments (e.g., `kubectl get deploy` showing >1 replicas) and that pods are distributed across different nodes or availability zones, often achieved using `topologySpreadConstraints` or observed via `kubectl get pods -o wide`. Operationally, during chaos tests, service uptime must remain unaffected, which can be verified through synthetic probes or Prometheus alerts, and crucially, recovery from disruptions must be entirely automatic, requiring no manual intervention."
    },
    {
      "user_input": "How does the 'Single Pod Deletion' scenario specifically validate the resilience mechanisms of Kubernetes, and what is the expected recovery timing for stateless applications in this context?",
      "generated_answer": "The \"Single Pod Deletion\" scenario in Kubernetes validates the resilience mechanisms of the platform by testing whether a single deleted pod is automatically replaced by the ReplicaSet or Deployment. The expected recovery timing for stateless applications in this context is typically less than 10 seconds, as observed in Krkn telemetry output. This indicates that the system has enough resources and policies to recover gracefully in the event of a single pod deletion.\n\nThe scenario is important because it simulates unplanned deletion of a single pod, which can occur due to various reasons such as node crashes or AZ outages. By testing whether the system can automatically create a replacement pod, the \"Single Pod Deletion\" scenario helps ensure continuous service even if a pod unexpectedly crashes.\n\nIn addition, the HA indicator for this scenario is whether the application can continue functioning from other replicas across zones/nodes after all pods of a service fail. If the system can recover gracefully and without manual intervention, it indicates that the HA mechanisms are working as expected.",
      "retrieved_context": [
        "Why  pod  scenarios  are  important:   \n \nUse  Cases  and  Importance  of  Pod-Level  Chaos  Scenarios  \n1.  Single  Pod  Deletion  \u25cf  Use  Case:  Simulates  unplanned  deletion  of  a  single  pod  \u25cf  Why  It's  Important:  Validates  whether  the  ReplicaSet  or  Deployment  automatically  \ncreates\n \na\n \nreplacement.\n \u25cf  Customer  Impact:  Ensures  continuous  service  even  if  a  pod  unexpectedly  crashes.  \u25cf  Recovery  Timing:  Typically  <10  seconds  for  stateless  apps  (seen  in  Krkn  telemetry  \noutput).\n \u25cf  HA  Indicator:  Pod  is  automatically  rescheduled  and  becomes  Ready  without  manual  \nintervention.\n  Kubectl  delete  pod  <pod-name> delete  pod  <pod-name>  Kubectl  get  pods  #  watch  for  new  pods \n2.  Multiple  Pods  Deleted  Simultaneously  \u25cf  Use  Case:  Simulates  a  larger  failure  event,  such  as  a  node  crash  or  AZ  outage.  \u25cf  Why  It's  Important:  Tests  whether  the  system  has  enough  resources  and  policies  to  \nrecover",
        "recover\n \ngracefully.\n \u25cf  Customer  Impact:  If  all  pods  of  a  service  fail,  user  experience  is  directly  impacted.  \u25cf  HA  Indicator:  Application  can  continue  functioning  from  other  replicas  across  \nzones/nodes.\n \n \n3.  Pod  Eviction  (Soft  Disruption)  \u25cf  Use  Case:  Triggered  by  Kubernetes  itself  during  node  upgrades  or  scaling  down.  \u25cf  Why  It's  Important:  Ensures  graceful  termination  and  restart  elsewhere  without  user  \nimpact.\n \u25cf  Customer  Impact:  Should  be  zero  if  readiness/liveness  probes  and  PDBs  are  correctly  \nconfigured.\n \u25cf  HA  Indicator:  Rolling  disruption  does  not  take  down  the  whole  application.  \n \nHow  to  Know  If  It's  Highly  Available",
        "1.  Multiple  Replicas  Exist:  Confirmed  by  checking  kubectl  get  deploy and  seeing  >1  \nreplicas.\n 2.  Pods  Distributed  Across  Nodes/availability  zones:  Using  topologySpreadConstraints or  observing  pod  distribution  in  kubectl  get  pods  \n-o\n \nwide.  3.  Service  Uptime  Remains  Unaffected:  During  chaos  test,  verify  app  availability  (  \nsynthetic\n \nprobes,\n \nPrometheus\n \nalerts,\n \netc).\n 4.  Recovery  Is  Automatic:  No  manual  intervention  needed  to  restore  service.  5.  Krkn  Telemetry  Indicators:  End  of  run  data  includes  recovery  times,  pod  reschedule  \nlatency,\n \nand\n \nservice\n \ndowntime\n \nwhich\n \nare\n \nvital\n \nmetrics\n \nfor\n \nassessing\n \nHA.",
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario"
      ],
      "reference_answer": "The 'Single Pod Deletion' scenario specifically validates whether the ReplicaSet or Deployment automatically creates a replacement pod when an individual pod is unexpectedly deleted. This mechanism ensures continuous service even if a single pod crashes. For stateless applications, the expected recovery timing in this scenario is typically less than 10 seconds, a performance metric often observed and confirmed through Krkn telemetry output."
    },
    {
      "user_input": "What is the primary concern regarding customer impact when 'Multiple Pods Deleted Simultaneously' occurs, and what architectural characteristic helps mitigate this impact to ensure high availability?",
      "generated_answer": "When \"Multiple Pods Deleted Simultaneously\" occurs, the primary concern regarding customer impact is ensuring continuous service even if multiple pods fail simultaneously. To mitigate this impact and ensure high availability, Kubernetes provides architectural characteristics such as ReplicaSets and Deployments, which automatically create replacement pods when a specified number of replicas are deleted. This ensures that the application can continue functioning without manual intervention, even in the event of simultaneous pod deletion.\n\nIn addition, Kubernetes provides HA indicators such as readiness and liveness probes, which help ensure that the system has enough resources and policies to recover gracefully in the event of a failure. These probes can be configured to detect when a pod is no longer running and take action to reschedule it or create a new replica.\n\nTo know if a Kubernetes cluster is highly available, you can check that multiple replicas exist, that pods are distributed across nodes/availability zones, that service uptime remains unaffected during chaos tests, and that recovery is automatic without manual intervention. You can also monitor end-of-run data for metrics such as recovery times, pod reschedule latency, and service downtime, which are vital indicators of high availability.",
      "retrieved_context": [
        "Why  pod  scenarios  are  important:   \n \nUse  Cases  and  Importance  of  Pod-Level  Chaos  Scenarios  \n1.  Single  Pod  Deletion  \u25cf  Use  Case:  Simulates  unplanned  deletion  of  a  single  pod  \u25cf  Why  It's  Important:  Validates  whether  the  ReplicaSet  or  Deployment  automatically  \ncreates\n \na\n \nreplacement.\n \u25cf  Customer  Impact:  Ensures  continuous  service  even  if  a  pod  unexpectedly  crashes.  \u25cf  Recovery  Timing:  Typically  <10  seconds  for  stateless  apps  (seen  in  Krkn  telemetry  \noutput).\n \u25cf  HA  Indicator:  Pod  is  automatically  rescheduled  and  becomes  Ready  without  manual  \nintervention.\n  Kubectl  delete  pod  <pod-name> delete  pod  <pod-name>  Kubectl  get  pods  #  watch  for  new  pods \n2.  Multiple  Pods  Deleted  Simultaneously  \u25cf  Use  Case:  Simulates  a  larger  failure  event,  such  as  a  node  crash  or  AZ  outage.  \u25cf  Why  It's  Important:  Tests  whether  the  system  has  enough  resources  and  policies  to  \nrecover",
        "recover\n \ngracefully.\n \u25cf  Customer  Impact:  If  all  pods  of  a  service  fail,  user  experience  is  directly  impacted.  \u25cf  HA  Indicator:  Application  can  continue  functioning  from  other  replicas  across  \nzones/nodes.\n \n \n3.  Pod  Eviction  (Soft  Disruption)  \u25cf  Use  Case:  Triggered  by  Kubernetes  itself  during  node  upgrades  or  scaling  down.  \u25cf  Why  It's  Important:  Ensures  graceful  termination  and  restart  elsewhere  without  user  \nimpact.\n \u25cf  Customer  Impact:  Should  be  zero  if  readiness/liveness  probes  and  PDBs  are  correctly  \nconfigured.\n \u25cf  HA  Indicator:  Rolling  disruption  does  not  take  down  the  whole  application.  \n \nHow  to  Know  If  It's  Highly  Available",
        "1.  Multiple  Replicas  Exist:  Confirmed  by  checking  kubectl  get  deploy and  seeing  >1  \nreplicas.\n 2.  Pods  Distributed  Across  Nodes/availability  zones:  Using  topologySpreadConstraints or  observing  pod  distribution  in  kubectl  get  pods  \n-o\n \nwide.  3.  Service  Uptime  Remains  Unaffected:  During  chaos  test,  verify  app  availability  (  \nsynthetic\n \nprobes,\n \nPrometheus\n \nalerts,\n \netc).\n 4.  Recovery  Is  Automatic:  No  manual  intervention  needed  to  restore  service.  5.  Krkn  Telemetry  Indicators:  End  of  run  data  includes  recovery  times,  pod  reschedule  \nlatency,\n \nand\n \nservice\n \ndowntime\n \nwhich\n \nare\n \nvital\n \nmetrics\n \nfor\n \nassessing\n \nHA.",
        "config\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario \nKill  a  pod.  :heavy_check_mark:  \nEtcd Kills  a  single/multiple  etcd  replicas.  \n:heavy_check_mark:"
      ],
      "reference_answer": "The primary concern regarding customer impact when 'Multiple Pods Deleted Simultaneously' occurs is that if all pods of a service fail, the user experience is directly impacted, potentially leading to service unavailability. This significant impact can be mitigated and high availability ensured if the application is designed with redundancy, allowing it to continue functioning from other replicas that are distributed across different zones or nodes, preventing a single point of failure from taking down the entire service."
    },
    {
      "user_input": "Describe the role of `topologySpreadConstraints` in achieving high availability and how their effective implementation can be verified using standard Kubernetes commands.",
      "generated_answer": "TopologySpreadConstraints is a feature in Kubernetes that allows for the spread of pods across multiple nodes or availability zones to achieve high availability. The effective implementation of TopologySpreadConstraints can be verified using standard Kubernetes commands such as `kubectl get deploy` and `kubectl get pods`.\n\nTo verify the deployment of pods across multiple nodes or availability zones, you can use the following commands:\n```\nkubectl get deploy -o wide\nkubectl get pods -o wide\n```\nThese commands will show you the distribution of pods across different nodes or availability zones. If TopologySpreadConstraints is enabled and properly configured, you should see a consistent spread of pods across multiple nodes or availability zones.\n\nAdditionally, you can use `kubectl get pods -o wide` to verify that the pods are running on multiple nodes or availability zones. If the pods are not running on multiple nodes or availability zones, it could be a sign that TopologySpreadConstraints is not enabled or properly configured.\n\nTo check if the recovery time of a pod is within the expected range, you can use `kubectl get pod <pod-name> -o jsonpath={{.status.phase}}`. If the phase of the pod is \"Running\" or \"Succeeded\", it means that the pod has recovered successfully. You can also use `kubectl get pods -o wide` to verify that all the pods have recovered within the expected time frame.\n\nOverall, TopologySpreadConstraints is an important feature in Kubernetes for achieving high availability, and its effective implementation can be verified using standard Kubernetes commands.",
      "retrieved_context": [
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "config\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario \nKill  a  pod.  :heavy_check_mark:  \nEtcd Kills  a  single/multiple  etcd  replicas.  \n:heavy_check_mark:",
        "1.  Multiple  Replicas  Exist:  Confirmed  by  checking  kubectl  get  deploy and  seeing  >1  \nreplicas.\n 2.  Pods  Distributed  Across  Nodes/availability  zones:  Using  topologySpreadConstraints or  observing  pod  distribution  in  kubectl  get  pods  \n-o\n \nwide.  3.  Service  Uptime  Remains  Unaffected:  During  chaos  test,  verify  app  availability  (  \nsynthetic\n \nprobes,\n \nPrometheus\n \nalerts,\n \netc).\n 4.  Recovery  Is  Automatic:  No  manual  intervention  needed  to  restore  service.  5.  Krkn  Telemetry  Indicators:  End  of  run  data  includes  recovery  times,  pod  reschedule  \nlatency,\n \nand\n \nservice\n \ndowntime\n \nwhich\n \nare\n \nvital\n \nmetrics\n \nfor\n \nassessing\n \nHA."
      ],
      "reference_answer": "`topologySpreadConstraints` play a crucial role in achieving high availability by ensuring that pods are distributed evenly across different failure domains, such as nodes or availability zones. This prevents a single point of failure from impacting all replicas of an application. The effective implementation and resulting distribution can be observed using standard Kubernetes commands like `kubectl get pods -o wide`, which displays the node on which each pod is running, allowing an administrator to visually confirm the desired spread across the cluster's topology."
    },
    {
      "user_input": "In the context of chaos engineering, what does 'Recovery Is Automatic' signify as a high availability indicator, and why is the absence of manual intervention considered critical for true HA?",
      "generated_answer": "In the context of chaos engineering, \"Recovery Is Automatic\" signifies a high availability indicator because it indicates that the system can automatically recover from failures without manual intervention. This is critical for true HA because it ensures that the system can quickly and gracefully recover from failures, without impacting the user experience.\n\nIn Kubernetes, multiple replicas exist and are distributed across nodes/availability zones using topologySpreadConstraints or observing pod distribution in `kubectl get pods -o wide`. Service uptime remains unaffected during chaos tests, as confirmed by synthetic probes, Prometheus alerts, and other metrics. Recovery is automatic, with no manual intervention needed to restore service. Additionally, Krkn telemetry indicators include recovery times, pod reschedule latency, and service downtime, which are vital metrics for assessing HA.\n\nThe absence of manual intervention is crucial because it ensures that the system can recover gracefully without user impact. If all pods of a service fail, customer impact is directly affected, and manual intervention may be necessary to restore service. However, with automatic recovery, the system can continue functioning from other replicas across zones/nodes, ensuring that the application remains available even in the face of failures.\n\nTo know if a system is highly available, chaos scenarios such as single pod deletion and multiple pods deleted simultaneously are important to test. These scenarios help validate whether the ReplicaSet or Deployment automatically creates replacement pods, ensuring continuous service even if a pod unexpectedly crashes. The recovery timing for stateless apps typically takes less than 10 seconds, as seen in Krkn telemetry output. Additionally, pod eviction (soft disruption) is important to ensure graceful termination and restart elsewhere without user impact.",
      "retrieved_context": [
        "1.  Multiple  Replicas  Exist:  Confirmed  by  checking  kubectl  get  deploy and  seeing  >1  \nreplicas.\n 2.  Pods  Distributed  Across  Nodes/availability  zones:  Using  topologySpreadConstraints or  observing  pod  distribution  in  kubectl  get  pods  \n-o\n \nwide.  3.  Service  Uptime  Remains  Unaffected:  During  chaos  test,  verify  app  availability  (  \nsynthetic\n \nprobes,\n \nPrometheus\n \nalerts,\n \netc).\n 4.  Recovery  Is  Automatic:  No  manual  intervention  needed  to  restore  service.  5.  Krkn  Telemetry  Indicators:  End  of  run  data  includes  recovery  times,  pod  reschedule  \nlatency,\n \nand\n \nservice\n \ndowntime\n \nwhich\n \nare\n \nvital\n \nmetrics\n \nfor\n \nassessing\n \nHA.",
        "recover\n \ngracefully.\n \u25cf  Customer  Impact:  If  all  pods  of  a  service  fail,  user  experience  is  directly  impacted.  \u25cf  HA  Indicator:  Application  can  continue  functioning  from  other  replicas  across  \nzones/nodes.\n \n \n3.  Pod  Eviction  (Soft  Disruption)  \u25cf  Use  Case:  Triggered  by  Kubernetes  itself  during  node  upgrades  or  scaling  down.  \u25cf  Why  It's  Important:  Ensures  graceful  termination  and  restart  elsewhere  without  user  \nimpact.\n \u25cf  Customer  Impact:  Should  be  zero  if  readiness/liveness  probes  and  PDBs  are  correctly  \nconfigured.\n \u25cf  HA  Indicator:  Rolling  disruption  does  not  take  down  the  whole  application.  \n \nHow  to  Know  If  It's  Highly  Available",
        "Why  pod  scenarios  are  important:   \n \nUse  Cases  and  Importance  of  Pod-Level  Chaos  Scenarios  \n1.  Single  Pod  Deletion  \u25cf  Use  Case:  Simulates  unplanned  deletion  of  a  single  pod  \u25cf  Why  It's  Important:  Validates  whether  the  ReplicaSet  or  Deployment  automatically  \ncreates\n \na\n \nreplacement.\n \u25cf  Customer  Impact:  Ensures  continuous  service  even  if  a  pod  unexpectedly  crashes.  \u25cf  Recovery  Timing:  Typically  <10  seconds  for  stateless  apps  (seen  in  Krkn  telemetry  \noutput).\n \u25cf  HA  Indicator:  Pod  is  automatically  rescheduled  and  becomes  Ready  without  manual  \nintervention.\n  Kubectl  delete  pod  <pod-name> delete  pod  <pod-name>  Kubectl  get  pods  #  watch  for  new  pods \n2.  Multiple  Pods  Deleted  Simultaneously  \u25cf  Use  Case:  Simulates  a  larger  failure  event,  such  as  a  node  crash  or  AZ  outage.  \u25cf  Why  It's  Important:  Tests  whether  the  system  has  enough  resources  and  policies  to  \nrecover",
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:"
      ],
      "reference_answer": "In chaos engineering, 'Recovery Is Automatic' as a high availability indicator signifies that the system is designed to self-heal and restore service without any human intervention after a disruption. The absence of manual intervention is considered critical for true HA because it demonstrates the system's inherent resilience and automation capabilities. Manual intervention introduces delays, increases the potential for human error, and indicates a dependency on operational teams rather than robust, self-managing infrastructure, undermining the goal of continuous service availability."
    },
    {
      "user_input": "What is the primary purpose of the `pod_disruption_scenarios` section within a Krkn configuration, and how is a specific scenario file referenced?",
      "generated_answer": "The primary purpose of the `pod_disruption_scenarios` section within a Krkn configuration is to define scenarios for disrupting pods in a Kubernetes/OpenShift cluster. This can be done by specifying a YAML file containing the scenario details, such as the targeted namespace, label selector, and disruption count. The `pod_disruption_scenarios` section allows users to create and manage chaos scenarios for their Kubernetes/OpenShift cluster using Krkn.\n\nTo reference a specific scenario file, simply provide the path to the YAML file within the `pod_disruption_scenarios` section of the configuration file. For example:\n```\npod_disruption_scenarios:\n  - path/to/scenario.yaml\n```\nThis will reference the specified YAML file as the scenario to be executed by Krkn.",
      "retrieved_context": [
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "scenario/faults\n \nbeing\n \ninjected:\n Example  if  \u2013env-host  is  used:  \nexport  <parameter_name>=<value>  \nOR  on  the  command  line  like  example:  \n-e  <VARIABLE>=<value>   \nSee  list  of  variables  that  apply  to  all  scenarios  here that  can  be  used/set  in  addition  to  these  scenario  specific  variables  Parameter  Description  Default  \nNAMESPACE  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nopenshift-.*  \nPOD_LABEL  Label  of  the  pod(s)  to  target  \n\"\"  \nNAME_PATTERN  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \n.*"
      ],
      "reference_answer": "The `pod_disruption_scenarios` section in a Krkn configuration is used to define and include specific pod chaos scenarios. A scenario file is referenced by providing its `path/to/scenario.yaml` within this section, allowing Krkn to execute the disruption defined in that file."
    },
    {
      "user_input": "How can a user leverage the provided schema file to enhance their experience when creating a Krkn scenario configuration in an IDE?",
      "generated_answer": "To leverage the provided schema file and enhance the experience when creating a Krkn scenario configuration in an IDE, you can use the schema reference to give your IDE code completion and documentation for the available options. Here are three ways to do this:\n\n1. Adjust the schema reference to point to the schema file: In your IDE, you can adjust the schema reference to point to the schema file provided. This will give you code completion and documentation for the available options in your IDE.\n2. Use the schema file for code completion: You can use the schema file to get code completion suggestions when writing your scenario configuration. For example, if you're writing a scenario that disrupts pods in a targeted namespace, you can use the schema file to get suggestions for the label selector and other parameters.\n3. Use the schema file for documentation: The schema file provides detailed documentation for each parameter and option available in Krkn scenarios. You can use this documentation to understand the different options and parameters available when creating your scenario configuration.\n\nOverall, leveraging the provided schema file can enhance your experience when creating a Krkn scenario configuration in an IDE by providing code completion and documentation suggestions, making it easier and faster to create and test your scenarios.",
      "retrieved_context": [
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "scenario/faults\n \nbeing\n \ninjected:\n Example  if  \u2013env-host  is  used:  \nexport  <parameter_name>=<value>  \nOR  on  the  command  line  like  example:  \n-e  <VARIABLE>=<value>   \nSee  list  of  variables  that  apply  to  all  scenarios  here that  can  be  used/set  in  addition  to  these  scenario  specific  variables  Parameter  Description  Default  \nNAMESPACE  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nopenshift-.*  \nPOD_LABEL  Label  of  the  pod(s)  to  target  \n\"\"  \nNAME_PATTERN  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \n.*"
      ],
      "reference_answer": "Users can enhance their experience by adjusting the schema reference in their scenario file to point to the `plugin.schema.json` file (e.g., `yaml-language-server: $schema=../plugin.schema.json`). This integration provides valuable features like code completion and documentation for available options directly within their IDE."
    },
    {
      "user_input": "In a kill-pods chaos scenario targeting Kubernetes components, which component is commonly disrupted and how are its pods typically selected?",
      "generated_answer": "The component that is commonly disrupted in a kill-pods chaos scenario is the pods in the targeted namespace. The pods are typically selected based on their label, with the most common label being \"k8s-app=kube-scheduler\". This is because the scheduler pod is responsible for scheduling and managing the execution of other pods in the cluster, and disrupting it can cause significant problems in the overall system.",
      "retrieved_context": [
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "config\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario \nKill  a  pod.  :heavy_check_mark:  \nEtcd Kills  a  single/multiple  etcd  replicas.  \n:heavy_check_mark:"
      ],
      "reference_answer": "The `kill-pods` scenario targets the `kube-scheduler` component. It selects pods by looking for those within the `kube-system` namespace (specified by `namespace_pattern: ^kube-system$`) that also possess the label `k8s-app=kube-scheduler` (specified by `label_selector`)."
    },
    {
      "user_input": "In a Krkn pod disruption scenario, what does the `krkn_pod_recovery_time` parameter control, and what is a typical value for it?",
      "generated_answer": "The `krkn_pod_recovery_time` parameter controls the timeout in seconds for which the pods are allowed to recover after a disruption. A typical value for this parameter is around 120 seconds.",
      "retrieved_context": [
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy"
      ],
      "reference_answer": "The `krkn_pod_recovery_time` parameter specifies the duration, in seconds, that Krkn will wait for a disrupted pod to recover. A commonly used value is `120`, which represents a 120\u2011second recovery period."
    },
    {
      "user_input": "Beyond a general 'basic pod scenario,' what are some of the critical Kubernetes or OpenShift components for which Krkn offers pre-defined chaos scenarios, and are they currently functional?",
      "generated_answer": "Krkn offers pre-defined chaos scenarios for disrupting pods in Kubernetes and OpenShift clusters. These scenarios include:\n\n1. Pod disruption scenarios: These scenarios disrupt a specific number of pods in a targeted namespace, based on the label selector specified.\n2. Kill pods scenario: This scenario kills a specific number of pods within a given timeout.\n3. Expected recovery time scenario: This scenario fails if the disrupted pods do not recover within a specified timeout.\n\nThe available options for these scenarios can be seen by running `krknctl run pod-scenarios --help`. Additionally, Cerberus can be enabled to monitor the cluster and pass/fail the scenario post-chaos.\n\nNote that these scenarios are pre-defined and may not cover all possible use cases. Custom scenarios can also be created using the `krknctl` command-line tool.",
      "retrieved_context": [
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:"
      ],
      "reference_answer": "Krkn offers pre-defined chaos scenarios for critical components such as Etcd, Kube ApiServer, ApiServer, Prometheus, and random pods running in OpenShift system namespaces. These scenarios are listed as functional in the Krkn documentation."
    },
    {
      "user_input": "If a user wanted to specifically target the Kubernetes API server for a chaos experiment, which Krkn scenario would be appropriate, and what action would it perform?",
      "generated_answer": "To target the Kubernetes API server for a chaos experiment using Krkn, you can use the `kill-pods` scenario. This scenario disrupts the API server replicas in the targeted namespace by killing them.\n\nHere's an example configuration file for the `kill-pods` scenario:\n```yaml\n# yaml-language-server: $schema=../plugin.schema.json\n# id: kill-pods\nconfig:\n  namespace_pattern: ^kube-system$\n  label_selector: k8s-app=kube-scheduler\n  krkn_pod_recovery_time: 120\n```\nIn this example, the `namespace_pattern` is set to `^kube-system$`, which means the scenario will target the `kube-system` namespace. The `label_selector` is set to `k8s-app=kube-scheduler`, which means the scenario will target pods with the label `k8s-app=kube-scheduler` in the `kube-system` namespace. The `krkn_pod_recovery_time` is set to 120, which means the scenario will wait for 120 seconds before considering the disruption complete.\n\nYou can run this scenario using the `krknctl run pod-scenarios` command, followed by the `--<parameter>:<value>` options if you want to specify any parameters. For example:\n```bash\nkrknctl run pod-scenarios --namespace=kube-system --kill-timeout=180\n```\nThis will run the `kill-pods` scenario in the `kube-system` namespace with a kill timeout of 180 seconds.",
      "retrieved_context": [
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "Kube  ApiServer Kills  a  single/multiple  kube-apiserver  replicas.  \n:heavy_check_mark:  \nApiServer Kills  a  single/multiple  apiserver  replicas.  \n:heavy_check_mark:  \nPrometheus Kills  a  single/multiple  prometheus  replicas.  \n:heavy_check_mark:  \nOpenShift  System  Pods \nKills  random  pods  running  in  the  OpenShift  system  namespaces.  \n:heavy_check_mark:"
      ],
      "reference_answer": "To target the Kubernetes API server, the 'Kube ApiServer' scenario would be appropriate. This scenario is designed to kill either a single or multiple replicas of the kube-apiserver component."
    },
    {
      "user_input": "How does the 'OpenShift System Pods' chaos scenario differ in its targeting approach compared to component-specific scenarios like 'Etcd' or 'Prometheus'?",
      "generated_answer": "The \"OpenShift System Pods\" chaos scenario differs from component-specific scenarios like \"Etcd\" or \"Prometheus\" in its targeting approach. While the latter two focus on specific components, \"OpenShift System Pods\" targets the entire Kubernetes/OpenShift cluster. This means that it can disrupt any pod in the cluster that matches the specified label, regardless of the component it belongs to.\n\nIn contrast, component-specific scenarios only target specific components, such as Etcd or Prometheus. These scenarios are designed to test the resilience and recovery of those components specifically, rather than the entire cluster.\n\nAdditionally, the \"OpenShift System Pods\" scenario provides more flexibility in terms of label selection. While component-specific scenarios typically only allow for a single label to be selected, \"OpenShift System Pods\" allows for multiple labels to be specified using the `--pod-label` parameter. This makes it possible to test different scenarios involving different labels and their impact on the cluster.",
      "retrieved_context": [
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "scenario/faults\n \nbeing\n \ninjected:\n Example  if  \u2013env-host  is  used:  \nexport  <parameter_name>=<value>  \nOR  on  the  command  line  like  example:  \n-e  <VARIABLE>=<value>   \nSee  list  of  variables  that  apply  to  all  scenarios  here that  can  be  used/set  in  addition  to  these  scenario  specific  variables  Parameter  Description  Default  \nNAMESPACE  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nopenshift-.*  \nPOD_LABEL  Label  of  the  pod(s)  to  target  \n\"\"  \nNAME_PATTERN  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \n.*"
      ],
      "reference_answer": "The 'OpenShift System Pods' scenario targets random pods running across various OpenShift system namespaces, providing a broader, less specific disruption. In contrast, component-specific scenarios like 'Etcd' or 'Prometheus' precisely target replicas of those named components, focusing disruption on a particular service."
    },
    {
      "user_input": "What is the purpose of the `id` field within a Krkn scenario configuration?",
      "generated_answer": "The `id` field within a Krkn scenario configuration is used to specify the unique identifier for the scenario. This identifier is used to identify the scenario in the Krkn hub and to differentiate it from other scenarios. The `id` field can be any valid string, but it must be unique within the scope of the Krkn hub.\n\nFor example, in the provided configuration, the `id` field is set to \"kill-pods\". This means that this scenario is specifically designed to disrupt pods in a particular namespace, based on a label selector. The `id` field is an important aspect of scenario configuration, as it allows Krkn to properly identify and manage each scenario within the hub.",
      "retrieved_context": [
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "scenario/faults\n \nbeing\n \ninjected:\n Example  if  \u2013env-host  is  used:  \nexport  <parameter_name>=<value>  \nOR  on  the  command  line  like  example:  \n-e  <VARIABLE>=<value>   \nSee  list  of  variables  that  apply  to  all  scenarios  here that  can  be  used/set  in  addition  to  these  scenario  specific  variables  Parameter  Description  Default  \nNAMESPACE  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nopenshift-.*  \nPOD_LABEL  Label  of  the  pod(s)  to  target  \n\"\"  \nNAME_PATTERN  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \n.*",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy"
      ],
      "reference_answer": "The `id` field, serves as a unique identifier for a specific chaos scenario defined within the Krkn configuration file. It provides a distinct name for the scenario being executed."
    },
    {
      "user_input": "Can the `kill-pods` scenario be configured to disrupt pods outside of the `kube-system` namespace?",
      "generated_answer": "The `kill-pods` scenario can be configured to disrupt pods outside of the `kube-system` namespace by using the `--namespace` parameter and specifying a non-empty value. For example, to disrupt pods in any namespace, you can use the following command:\n```\nkrknctl run pod-scenarios --namespace ()\n```\nThis will disrupt all pods in any namespace that matches the label specified in the `--pod-label` parameter.\n\nAlternatively, you can use the `--random-namespaces` parameter to disrupt random pods in any namespace. For example:\n```\nkrknctl run pod-scenarios --random-namespaces ()\n```\nThis will disrupt a random number of pods in any namespace that matches the label specified in the `--pod-label` parameter.\n\nNote that the `kill-pods` scenario can only be used to disrupt pods in a Kubernetes/OpenShift cluster, and it cannot be used to disrupt pods in other environments or platforms.",
      "retrieved_context": [
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "Kube  ApiServer Kills  a  single/multiple  kube-apiserver  replicas.  \n:heavy_check_mark:  \nApiServer Kills  a  single/multiple  apiserver  replicas.  \n:heavy_check_mark:  \nPrometheus Kills  a  single/multiple  prometheus  replicas.  \n:heavy_check_mark:  \nOpenShift  System  Pods \nKills  random  pods  running  in  the  OpenShift  system  namespaces.  \n:heavy_check_mark:",
        "Shell \nDISRUPTION_COUNT  Number  of  pods  to  disrupt  \n1  \nKILL_TIMEOUT  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \n180  \nEXPECTED_RECOVERY_TIME  Fails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \n120  \nNote  Set  NAMESPACE  environment  variable  to  openshift-.*  to  pick  and  disrupt  pods  randomly  in  \nopenshift\n \nsystem\n \nnamespaces,\n \nthe\n \nDAEMON_MODE\n \ncan\n \nalso\n \nbe\n \nenabled\n \nto\n \ndisrupt\n \nthe\n \npods\n \nevery\n \nx\n \nseconds\n \nin\n \nthe\n \nbackground\n \nto\n \ncheck\n \nthe\n \nreliability.\n \nNote  In  case  of  using  custom  metrics  profile  or  alerts  profile  when  CAPTURE_METRICS  or  \nENABLE_ALERTS\n \nis\n \nenabled,\n \nmount\n \nthe\n \nmetrics\n \nprofile\n \nfrom\n \nthe\n \nhost\n \non\n \nwhich\n \nthe\n \ncontainer\n \nis\n \nrun\n \nusing\n \npodman/docker\n \nunder\n \n/home/krkn/kraken/config/metrics-aggregated.yaml\n \nand\n \n/home/krkn/kraken/config/alerts.\n For  example:",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy"
      ],
      "reference_answer": "By default, the `kill-pods` scenario is restricted to the `kube-system` namespace through the `namespace_pattern: ^kube-system$` setting. However, this setting can be modified to target pods in other namespaces if desired."
    },
    {
      "user_input": "Describe the hierarchical structure for defining chaos scenarios within the top-level `kraken` configuration block.",
      "generated_answer": "To define chaos scenarios within the top-level `kraken` configuration block, you can use the `chaos_scenarios` block to specify a list of pod disruption scenarios. Each scenario can have its own configuration, including the namespace pattern, label selector, recovery time, and other parameters.\n\nHere is an example of how to define a basic chaos scenario:\n```\nkraken:\n  chaos_scenarios:\n    - pod_disruption_scenarios:\n          - path/to/scenario.yaml\n```\nYou can then create a separate YAML file with the scenario configuration, such as `scenario.yaml`, and define the parameters for the scenario. Here is an example of how to define a basic pod disruption scenario:\n```\nkube-system:\n  label_selector: k8s-app=kube-scheduler\n  namespace_pattern: ^kube-system$\n  krkn_pod_recovery_time: 120\n```\nYou can also set any global variables listed in the `Scenario specific parameters` section of the `krknctl run pod-scenarios` command.\n\nTo see all available scenario options, you can use the `--help` flag when running `krknctl run pod-scenarios`.\n\nNote that this is just an example, and you will need to modify it to suit your specific needs.",
      "retrieved_context": [
        "None \n None \nPod  Scenarios  using  Krkn  \nExample  Config  The  following  are  the  components  of  Kubernetes  for  which  a  basic  chaos  scenario  config  exists  \ntoday.\n \nkraken:    chaos_scenarios:      -  pod_disruption_scenarios:        -  path/to/scenario.yaml  \nCopy  You  can  then  create  the  scenario  file  with  the  following  contents:  \n#  yaml-language-server:  $schema=../plugin.schema.json  -  id:  kill-pods    config:      namespace_pattern:  ^kube-system$      label_selector:  k8s-app=kube-scheduler      krkn_pod_recovery_time:  120       \nCopy  Please  adjust  the  schema  reference  to  point  to  the  schema  file.  This  file  will  give  you  code  completion  and  documentation  for  the  available  options  in  your  IDE.  \nPod  Chaos  Scenarios  The  following  are  the  components  of  Kubernetes/OpenShift  for  which  a  basic  chaos  scenario  \nconfig\n \nexists\n \ntoday.\n Component  Description  Working  \nBasic  pod  scenario",
        "Shell \n Shell \nPod  Scenarios  using  Krknctl  \nkrknctl  run  pod-scenarios  (optional:  --<parameter>:<value>  )  \nCopy  Can  also  set  any  global  variable  listed  here Scenario  specific  parameters:  Parameter  Description  Type  Default  \n--namespace  Targeted  namespace  in  the  cluster  (  supports  regex  )  \nstring  openshift-*  \n--pod-label  Label  of  the  pod(s)  to  target  ex.  \u201capp=test\u201d  \nstring   \n--name-pattern  Regex  pattern  to  match  the  pods  in  NAMESPACE  when  POD_LABEL  is  not  specified  \nstring  .*  \n--disruption-count  Number  of  pods  to  disrupt  number  1  \n--kill-timeout  Timeout  to  wait  for  the  target  pod(s)  to  be  removed  in  seconds  \nnumber  180  \n--expected-recovery-time  \nFails  if  the  pod  disrupted  do  not  recover  within  the  timeout  set  \nnumber  120  \nTo  see  all  available  scenario  options  \nkrknctl  run  pod-scenarios  --help",
        "Shell \n Shell \nPod  Scenarios  using  Krkn-hub  \nThis  scenario  disrupts  the  pods  matching  the  label  in  the  specified  namespace  on  a  \nKubernetes/OpenShift\n \ncluster.\n \nRun  If  enabling  Cerberus to  monitor  the  cluster  and  pass/fail  the  scenario  post  chaos,  refer  docs.  Make  sure  to  start  it  before  injecting  the  chaos  and  set  CERBERUS_ENABLED  environment  \nvariable\n \nfor\n \nthe\n \nchaos\n \ninjection\n \ncontainer\n \nto\n \nautoconnect.\n \n$  podman  run  --name=<container_name>  --net=host  --env-host=true  -v  \n<path-to-kube-config>:/home/krkn/.kube/config:Z\n \n-d\n \ncontainers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n $  podman  logs  -f  <container_name  or  container_id>  #  Streams  Kraken  logs  $  podman  inspect  <container-name  or  container-id>  --format  \"{{.State.ExitCode}}\"  #  \nOutputs\n \nexit\n \ncode\n \nwhich\n \ncan\n \nconsidered\n \nas\n \npass/fail\n \nfor\n \nthe\n \nscenario\n \nCopy",
        "1.  Multiple  Replicas  Exist:  Confirmed  by  checking  kubectl  get  deploy and  seeing  >1  \nreplicas.\n 2.  Pods  Distributed  Across  Nodes/availability  zones:  Using  topologySpreadConstraints or  observing  pod  distribution  in  kubectl  get  pods  \n-o\n \nwide.  3.  Service  Uptime  Remains  Unaffected:  During  chaos  test,  verify  app  availability  (  \nsynthetic\n \nprobes,\n \nPrometheus\n \nalerts,\n \netc).\n 4.  Recovery  Is  Automatic:  No  manual  intervention  needed  to  restore  service.  5.  Krkn  Telemetry  Indicators:  End  of  run  data  includes  recovery  times,  pod  reschedule  \nlatency,\n \nand\n \nservice\n \ndowntime\n \nwhich\n \nare\n \nvital\n \nmetrics\n \nfor\n \nassessing\n \nHA."
      ],
      "reference_answer": "Within the `kraken` configuration block, chaos scenarios are organized under the `chaos_scenarios` key. This key contains a list, where each item is a `pod_disruption_scenarios` entry, which in turn specifies the `path/to/scenario.yaml` file containing the detailed configuration for a particular pod disruption."
    }
  ],
  "email": "user_email@example.com"
}