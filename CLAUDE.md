# CLAUDE.md
<!-- Generated by Claude Sonnet 4 -->

This file provides guidance to Claude Code when working with the krkn-lightspeed project.

## Project Overview

krkn-lightspeed is a RAG (Retrieval-Augmented Generation) chatbot for chaos engineering assistance. The project supports two operating modes:

1. **Default Mode**: Uses Ollama with LLaMA 3.1 (original)
2. **krknctl Mode**: Uses llama.cpp with pre-downloaded model (for krknctl integration)

## Architecture

### Shared RAG Pipeline
Both modes share the same core RAG logic:
- **Vector Database**: ChromaDB for document indexing
- **Embedding**: Configurable embedding models
- **State Graph**: LangGraph for RAG workflow orchestration
- **Documents**: KRKN documentation and chaos engineering scenarios

### Configurable LLM Backend
- **Factory Pattern**: `utils/llm_factory.py` creates different LLM instances
- **Ollama Backend**: For default mode (requires Ollama server)
- **llama.cpp Backend**: For krknctl mode (uses local model)

## Key Files

### New Files Added
- `fastapi_server.py`: FastAPI server compatible with OpenAI API
- `utils/llm_factory.py`: Factory for creating different LLM backends

### Modified Files
- `rag_pipelines/llama31_rag_pipeline.py`: Added `llm_backend` parameter
- `main.py`: Added `--krknctl` flag for backend selection
- `requirements.txt`: Added FastAPI dependencies

## Usage Modes

### 1. Default Mode (Ollama)
```bash
# Terminal interface
python3 main.py

# Streamlit UI
streamlit run app.py
```

### 2. krknctl Mode (llama.cpp)
```bash
# Terminal interface
python3 main.py --krknctl

# FastAPI server
python3 fastapi_server.py
```

### 3. FastAPI Server
```bash
# Start the server
python3 fastapi_server.py

# Test endpoint
curl -X POST "http://localhost:8000/v1/chat/completions" \
     -H "Content-Type: application/json" \
     -d '{
       "model": "llama",
       "messages": [{"role": "user", "content": "What is krkn?"}],
       "max_tokens": 512
     }'
```

## krknctl Integration

### Pre-downloaded Model
The model is automatically downloaded in krknctl's Containerfile:
- **Path**: `/app/models/Llama-3.2-1B-Instruct-Q4_K_M.gguf`
- **Model**: Llama-3.2-1B-Instruct quantized Q4_K_M
- **Size**: ~740MB

### Environment Variables
```bash
MODEL_PATH=/app/models/Llama-3.2-1B-Instruct-Q4_K_M.gguf
USE_GPU=true
PORT=8000
HOST=0.0.0.0
```

## API Endpoints

### POST /v1/chat/completions
OpenAI-compatible chat completions API endpoint.

**Request:**
```json
{
  "model": "llama",
  "messages": [
    {"role": "user", "content": "How can I configure krkn?"}
  ],
  "temperature": 0.7,
  "max_tokens": 512
}
```

**Response:**
```json
{
  "id": "chatcmpl-123456",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "llama",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "To configure krkn..."
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 50,
    "total_tokens": 60
  }
}
```

### POST /v1/completions
OpenAI-compatible completions API endpoint.

### GET /health
Health check endpoint for the service.

## Development

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Local Testing
To test krknctl mode locally, you need to:
1. Download the model manually:
```bash
mkdir -p models
python3 -c "
import huggingface_hub
huggingface_hub.hf_hub_download(
    repo_id='bartowski/Llama-3.2-1B-Instruct-GGUF',
    filename='Llama-3.2-1B-Instruct-Q4_K_M.gguf',
    local_dir='./models',
    local_dir_use_symlinks=False
)
"
```

2. Set the environment variable:
```bash
export MODEL_PATH=./models/Llama-3.2-1B-Instruct-Q4_K_M.gguf
```

### Directory Structure
```
krkn-lightspeed/
├── data/                    # Documents for RAG
├── rag_pipelines/          # RAG pipelines
│   ├── llama31_rag_pipeline.py
│   └── ...
├── utils/                  # Utilities
│   ├── llm_factory.py     # Factory for LLM backends (NEW)
│   └── ...
├── fastapi_server.py      # FastAPI server (NEW)
├── main.py               # Main CLI (MODIFIED)
├── app.py                # Streamlit UI (UNCHANGED)
└── requirements.txt      # Dependencies (UPDATED)
```

## Compatibility

### Backward Compatibility
- Default behavior remains identical to original implementation
- Streamlit UI works without modifications
- All existing pipelines continue to function

### Forward Compatibility
- New llama.cpp backend is completely isolated
- krknctl mode does not interfere with default mode
- Modular architecture facilitates adding new backends

## Branch
Changes have been implemented in the `krknctl_lightspeed` branch to maintain separation from original functionality.

## Notes for Claude
- Always use the `--krknctl` flag when working with llama.cpp backend
- The `utils/llm_factory.py` file centralizes backend creation logic
- The core RAG pipeline in `rag_pipelines/llama31_rag_pipeline.py` is shared across all modes
- The FastAPI server uses the same pipeline as the CLI, ensuring consistency