# Generated by Claude Code
"""
krknctl-compatible RAG pipeline using FAISS + llama.cpp + multi-qa-mpnet-base-dot-v1
Integrates with existing krkn-lightspeed structure
while using our optimized indexing system
"""

import os
import logging
from llama_cpp import Llama
from utils.faiss_document_indexer import FaissDocumentIndexer
from utils.faiss_vector_store import FAISSVectorStore, SimpleStateGraph
from utils.state_graph import State

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def detect_gpu_type():
    """Detect the type of GPU available and return
    appropriate llama.cpp parameters"""
    logger.info("Detecting GPU type for optimal llama.cpp backend...")

    # Log all device checks for debugging
    nvidia_dev0 = os.path.exists("/dev/nvidia0")
    nvidia_ctl = os.path.exists("/dev/nvidiactl")
    dri_card0 = os.path.exists("/dev/dri/card0")
    dri_render = os.path.exists("/dev/dri/renderD128")

    logger.info(f"Device detection results:")
    logger.info(f"  /dev/nvidia0: {nvidia_dev0}")
    logger.info(f"  /dev/nvidiactl: {nvidia_ctl}")
    logger.info(f"  /dev/dri/card0: {dri_card0}")
    logger.info(f"  /dev/dri/renderD128: {dri_render}")

    # Check for NVIDIA GPU devices
    if nvidia_dev0 or nvidia_ctl:
        logger.info("NVIDIA GPU devices detected - using CUDA backend")
        return {
            "backend": "cuda",
            "n_gpu_layers": -1,
            "main_gpu": 0,
            "verbose": False,
        }

    # Check for DRI devices (Apple Silicon or other GPUs with Vulkan)
    if dri_card0 or dri_render:
        logger.info("DRI devices detected - using Vulkan backend")
        return {"backend": "vulkan", "n_gpu_layers": -1, "verbose": False}

    # Fallback to CPU
    logger.info("No GPU detected - using CPU backend")
    cpu_count = os.cpu_count()
    logger.info(f"CPU threads available: {cpu_count}")
    return {
        "backend": "cpu",
        "n_gpu_layers": 0,
        "n_threads": cpu_count,
        "verbose": False,
    }


def load_llama31_krknctl_rag_pipeline(
    github_repo="https://github.com/krkn-chaos/website",
    repo_path="content/en/docs",
    persist_dir="faiss_index",
    model_path="models/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
    krkn_hub_repo="https://github.com/krkn-chaos/krkn-hub",
):
    """
    Load krknctl RAG pipeline using FAISS + llama.cpp + all-MiniLM-L6-v2

    Compatible with existing krkn-lightspeed state graph structure
    """

    logger.info("Loading krknctl RAG pipeline with FAISS backend")

    # Ensure directories exist
    os.makedirs(persist_dir, exist_ok=True)
    os.makedirs(os.path.dirname(model_path), exist_ok=True)

    # Check if index exists or if forced rebuild is requested
    index_path = os.path.join(persist_dir, "index.faiss")
    force_reindex = os.environ.get("FORCE_REINDEX", "false").lower() == "true"

    if not os.path.exists(index_path) or force_reindex:
        if force_reindex:
            logger.info("FORCE_REINDEX=true detected, rebuilding index...")
        else:
            logger.info("Index not found, building new index...")
        indexer = FaissDocumentIndexer()
        indexer.build_and_save_index(
            github_repo, repo_path, persist_dir, krkn_hub_repo
        )

    # Load vector store
    logger.info("Loading FAISS vector store...")
    vector_store = FAISSVectorStore(persist_dir)

    # Initialize Llama model
    llama_model = None
    if os.path.exists(model_path):
        logger.info(f"Loading Llama 3.2 3B model from {model_path}")

        gpu_config = detect_gpu_type()
        logger.info(f"Using GPU backend: {gpu_config['backend']}")

        llama_params = {
            "model_path": model_path,
            "n_ctx": 16384,  # Increased from 4096 to better utilize model capacity
            "n_gpu_layers": gpu_config["n_gpu_layers"],
            "verbose": gpu_config["verbose"],
        }

        # Add Vulkan-specific parameters if using Vulkan backend
        if gpu_config["backend"] == "vulkan":
            llama_params.update({
                "split_mode": 1,  # Vulkan-specific: layer split mode
                "tensor_split": None,  # Vulkan-specific: tensor split configuration
                "main_gpu": 0,  # Vulkan-specific: main GPU selection
                "offload_kqv": True,  # Vulkan-specific: offload key-query-value to GPU
            })
            logger.info("Added Vulkan-specific model parameters")

        if gpu_config["backend"] == "cuda":
            llama_params["main_gpu"] = gpu_config["main_gpu"]
        elif gpu_config["backend"] == "cpu":
            llama_params["n_threads"] = gpu_config["n_threads"]

        llama_model = Llama(**llama_params)
        logger.info("Llama 3.2 3B model loaded successfully")

        # Log detailed hardware configuration
        logger.info("=== HARDWARE CONFIGURATION ===")
        logger.info("ðŸ”„ CODE VERSION: 2024-10-31-v7-VULKAN-NATIVE") # Version marker
        logger.info(f"Backend: {gpu_config['backend']}")
        logger.info(f"GPU Layers: {gpu_config['n_gpu_layers']}")
        if 'main_gpu' in gpu_config:
            logger.info(f"Main GPU: {gpu_config['main_gpu']}")
        if 'n_threads' in gpu_config:
            logger.info(f"CPU Threads: {gpu_config['n_threads']}")
        logger.info(f"Model Context Size: {llama_params['n_ctx']}")
        logger.info("===============================")
    else:
        logger.warning(f"Llama model not found at {model_path}")

    # Build state graph compatible with existing structure
    def retrieve(state: State):
        """Retrieve relevant documents"""
        retrieved_docs = vector_store.similarity_search(state["question"], k=5)
        return {"context": retrieved_docs}

    def generate(state: State):
        """Generate response using llama.cpp with backend-optimized parameters"""
        if not llama_model:
            return {
                "answer": "Llama model not available. Please check model path."
            }

        # Build context from retrieved documents with
        # source information for model decision-making
        context_parts = []
        for i, doc in enumerate(state["context"]):
            relevance_score = doc.metadata.get("relevance_score", 0)
            source = doc.metadata.get("source", "unknown")
            context_parts.append(
                f"[RELEVANCE: {relevance_score:.3f} | "
                f"SOURCE: {source}] {doc.page_content}"
            )

        docs_content = "\n\n".join(context_parts)

        # Create strict prompt that enforces context-only responses
        prompt = f"""You are a chaos engineering assistant specialized in krknctl commands. Use ONLY the provided context to answer questions.

CONTEXT:
{docs_content}

QUESTION: {state["question"]}

CRITICAL RULES - FOLLOW STRICTLY:
1. SCENARIO NAMES: Extract scenario names ONLY from the provided context documents. Look for:
   - "krknctl run <scenario-name>" commands in examples
   - "# krknctl Scenario: <scenario-name>" headers
   - Command patterns in the retrieved documentation

2. COMMAND SYNTAX: krknctl run <exact-scenario-name> --<parameter> <value>

3. PARAMETER EXTRACTION: Use parameters ONLY if explicitly documented in the context with proper parameter descriptions

4. RESPONSE FORMAT: If recommending a scenario, start with: "SCENARIO: <exact-scenario-name-from-context>"

5. NEVER INVENT: Do not guess or create scenario names, parameters, or values not present in the context

6. CONTEXT DEPENDENCY: If the context doesn't contain the information needed to answer the question, say so explicitly

7. VERIFICATION: Before suggesting any command, verify both the scenario name and ALL parameters exist in the provided context documents

Answer based ONLY on the context provided:"""  # NOQA

        try:
            # Backend-optimized parameters for consistent quality across platforms
            backend = gpu_config.get("backend", "cpu")

            if backend == "cuda":
                # CUDA: Proven parameters
                inference_params = {
                    "max_tokens": 500,
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "repeat_penalty": 1.15,
                    "echo": False,
                }
                logger.info("Using standard CUDA parameters")

            elif backend == "vulkan":
                # Vulkan: Backend-specific parameters for consistent behavior
                inference_params = {
                    "max_tokens": 500,
                    "temperature": 0.0,      # Completely deterministic
                    "repeat_penalty": 1.15,  # Keep penalty
                    "seed": 42,              # Vulkan-specific: fixed seed for reproducibility
                    "tfs_z": 1.0,           # Vulkan-specific: tail free sampling disabled
                    "typical_p": 1.0,        # Vulkan-specific: typical sampling disabled
                    "echo": False,
                }
                logger.info("Using Vulkan-specific deterministic parameters")

            else:  # CPU
                # CPU: Conservative but effective
                inference_params = {
                    "max_tokens": 500,
                    "temperature": 0.08,
                    "top_p": 0.85,
                    "repeat_penalty": 1.2,
                    "top_k": 40,
                    "echo": False,
                }
                logger.info("Using CPU-optimized parameters")

            logger.info("=== INFERENCE PARAMETERS ===")
            logger.info("ðŸŽ¯ CUDA-MATCH VERSION ACTIVE") # Inference marker
            logger.info(f"Backend: {backend}")
            for param, value in inference_params.items():
                logger.info(f"{param}: {value}")
            logger.info(f"Prompt length: {len(prompt)} characters")
            logger.info("=============================")

            # Balanced parameters for accuracy without over-restriction
            response = llama_model(
                prompt,
                **inference_params
            )

            generated_response = response["choices"][0]["text"].strip()

            # Log inference performance metrics
            usage = response.get("usage", {})
            logger.info("=== INFERENCE METRICS ===")
            logger.info(f"Prompt tokens: {usage.get('prompt_tokens', 'unknown')}")
            logger.info(f"Completion tokens: {usage.get('completion_tokens', 'unknown')}")
            logger.info(f"Total tokens: {usage.get('total_tokens', 'unknown')}")
            if 'timings' in response:
                timings = response['timings']
                logger.info(f"Prompt eval time: {timings.get('prompt_eval_time', 'unknown')}ms")
                logger.info(f"Eval time: {timings.get('eval_time', 'unknown')}ms")
                logger.info(f"Tokens per second: {timings.get('tokens_per_second', 'unknown')}")
            logger.info("=========================")

            # DEBUG: Print generated response
            print("\n[DEBUG] GENERATED RESPONSE:")
            print(f"Query: {state['question']}")
            print(f"Response length: {len(generated_response)} characters")
            print(f"Response preview: {generated_response[:200]}...")
            if "SCENARIO:" in generated_response:
                print("SCENARIO tag detected - krknctl command response")
            else:
                print(
                    "No SCENARIO tag - likely theoretical "
                    "chaos testing response"
                )
            print("[DEBUG] Response generation completed\n")

            return {"answer": generated_response}

        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return {
                "answer": "I encountered an error while "
                "processing your request. Please try again."
            }

    # Create a simple state graph object that mimics langgraph behavior

    graph = SimpleStateGraph(retrieve, generate, vector_store)
    logger.info("krknctl RAG pipeline ready!")

    return graph
