# Generated by Claude Sonnet 4
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import uvicorn
from contextlib import asynccontextmanager
import os
import logging
from rag_pipelines.llama31_rag_pipeline import load_llama31_rag_pipeline

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global RAG pipeline instance
rag_pipeline = None

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512
    stream: Optional[bool] = False

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, int]

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize and cleanup the RAG pipeline"""
    global rag_pipeline
    
    logger.info("Loading RAG pipeline with llama.cpp backend...")
    
    try:
        # Use the same pipeline logic with llama.cpp backend
        rag_pipeline = load_llama31_rag_pipeline(llm_backend="llamacpp")
        logger.info("RAG pipeline loaded successfully")
    except Exception as e:
        logger.error(f"Failed to load RAG pipeline: {e}")
        raise
    
    yield
    
    # Cleanup
    if rag_pipeline:
        rag_pipeline = None
        logger.info("RAG pipeline cleaned up")

app = FastAPI(
    title="KRKN Lightspeed API",
    description="FastAPI server compatible with OpenAI API for KRKN chaos engineering assistance",
    version="1.0.0",
    lifespan=lifespan
)

@app.get("/")
async def root():
    return {"message": "KRKN Lightspeed API is running"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "rag_pipeline_loaded": rag_pipeline is not None}

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """OpenAI-compatible chat completions endpoint"""
    if rag_pipeline is None:
        raise HTTPException(status_code=503, detail="RAG pipeline not loaded")
    
    try:
        # Extract the user question from messages
        user_question = None
        for message in reversed(request.messages):
            if message.role == "user":
                user_question = message.content
                break
        
        if not user_question:
            raise HTTPException(status_code=400, detail="No user message found")
        
        # Use the RAG pipeline to generate response
        result = rag_pipeline.invoke({"question": user_question})
        response_text = result.get("answer", "").strip()
        
        # Format response in OpenAI format
        return ChatCompletionResponse(
            id=f"chatcmpl-{hash(user_question) % 10000000}",
            created=int(__import__("time").time()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }],
            usage={
                "prompt_tokens": len(user_question.split()),
                "completion_tokens": len(response_text.split()),
                "total_tokens": len(user_question.split()) + len(response_text.split())
            }
        )
    
    except Exception as e:
        logger.error(f"Error generating response: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/completions")
async def completions(request: dict):
    """OpenAI-compatible completions endpoint"""
    if rag_pipeline is None:
        raise HTTPException(status_code=503, detail="RAG pipeline not loaded")
    
    try:
        prompt = request.get("prompt", "")
        
        # Use RAG pipeline for completions too
        result = rag_pipeline.invoke({"question": prompt})
        response_text = result.get("answer", "").strip()
        
        return {
            "id": f"cmpl-{hash(prompt) % 10000000}",
            "object": "text_completion",
            "created": int(__import__("time").time()),
            "model": request.get("model", "llama"),
            "choices": [{
                "text": response_text,
                "index": 0,
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": len(prompt.split()),
                "completion_tokens": len(response_text.split()),
                "total_tokens": len(prompt.split()) + len(response_text.split())
            }
        }
    
    except Exception as e:
        logger.error(f"Error generating completion: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "0.0.0.0")
    
    uvicorn.run(app, host=host, port=port)