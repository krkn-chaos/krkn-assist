# Generated by Claude Sonnet 4
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import uvicorn
from contextlib import asynccontextmanager
import os
import logging
import subprocess
import re
from rag_pipelines.llama31_rag_pipeline import load_llama31_rag_pipeline

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global RAG pipeline instance
rag_pipeline = None

def extract_scenario_name(response_text: str) -> Optional[str]:
    """Extract scenario name from model response"""
    # Only look for explicit SCENARIO: pattern
    scenario_match = re.search(r'SCENARIO:\s*([a-zA-Z0-9_-]+)', response_text)
    if scenario_match:
        scenario_name = scenario_match.group(1)
        logger.info(f"✅ Scenario detected: {scenario_name}")
        return scenario_name

    logger.warning("⚠️ Scenario not found in the response - model did not follow SCENARIO: format")
    return None

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512
    stream: Optional[bool] = False

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, int]
    scenario_name: Optional[str] = None  # krknctl scenario name if detected

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize and cleanup the RAG pipeline"""
    global rag_pipeline
    
    logger.info("Loading RAG pipeline with llama.cpp backend...")
    
    try:
        # Use the same pipeline logic with llama.cpp backend and correct persist_dir
        rag_pipeline = load_llama31_rag_pipeline(
            llm_backend="llamacpp",
            persist_dir="/app/docs_index"
        )
        logger.info("RAG pipeline loaded successfully")
        logger.info("FastAPI service ready to accept requests")
    except Exception as e:
        logger.error(f"Failed to load RAG pipeline: {e}")
        logger.error(f"Exception type: {type(e).__name__}")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        raise
    
    yield
    
    # Cleanup
    if rag_pipeline:
        rag_pipeline = None
        logger.info("RAG pipeline cleaned up")

app = FastAPI(
    title="KRKN Lightspeed API",
    description="FastAPI server compatible with OpenAI API for KRKN chaos engineering assistance",
    version="1.0.0",
    lifespan=lifespan
)

@app.get("/")
async def root():
    return {"message": "KRKN Lightspeed API is running"}

@app.get("/debug/documents")
async def debug_documents():
    """Debug endpoint to inspect ChromaDB documents"""
    if rag_pipeline is None:
        raise HTTPException(status_code=503, detail="RAG pipeline not loaded")

    try:
        # Get the vector store from the pipeline
        vector_store = None

        # Try to access the vector store from the RAG pipeline state graph
        if hasattr(rag_pipeline, 'get_state') or hasattr(rag_pipeline, 'nodes'):
            # This is a StateGraph, we need to find the vector store
            logger.info("Attempting to access vector store from state graph...")
            # For now, let's create a direct connection to ChromaDB

        # Direct ChromaDB connection for debugging
        import chromadb
        client = chromadb.PersistentClient(path="/app/docs_index")
        collections = client.list_collections()

        result = {
            "collections": [],
            "total_documents": 0
        }

        for collection in collections:
            collection_info = {
                "name": collection.name,
                "count": collection.count(),
                "metadata": collection.metadata if hasattr(collection, 'metadata') else None,
                "sample_documents": []
            }

            # Get sample documents
            if collection.count() > 0:
                sample_results = collection.get(limit=5, include=['documents', 'metadatas'])
                for i, doc in enumerate(sample_results.get('documents', [])):
                    metadata = sample_results.get('metadatas', [{}])[i] if sample_results.get('metadatas') else {}
                    collection_info["sample_documents"].append({
                        "content_preview": doc[:200] + "..." if len(doc) > 200 else doc,
                        "metadata": metadata
                    })

            result["collections"].append(collection_info)
            result["total_documents"] += collection.count()

        return result

    except Exception as e:
        logger.error(f"Error inspecting documents: {e}")
        raise HTTPException(status_code=500, detail=f"Error inspecting documents: {str(e)}")

@app.get("/debug/document_sources")
async def debug_document_sources():
    """Debug endpoint to see document sources breakdown"""
    if rag_pipeline is None:
        raise HTTPException(status_code=503, detail="RAG pipeline not loaded")

    try:
        import chromadb
        client = chromadb.PersistentClient(path="/app/docs_index")
        collection = client.get_collection("krkn-docs")

        # Get all documents with metadata
        all_results = collection.get(include=['metadatas'])
        metadatas = all_results.get('metadatas', [])

        # Analyze sources
        sources = {}
        for metadata in metadatas:
            source = metadata.get('source', 'unknown') if metadata else 'unknown'
            sources[source] = sources.get(source, 0) + 1

        return {
            "total_documents": len(metadatas),
            "sources_breakdown": sources,
            "sample_metadata": metadatas[:10] if metadatas else []
        }

    except Exception as e:
        logger.error(f"Error analyzing document sources: {e}")
        raise HTTPException(status_code=500, detail=f"Error analyzing document sources: {str(e)}")

@app.get("/health")
async def health_check():
    return {"status": "healthy", "rag_pipeline_loaded": rag_pipeline is not None}

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """OpenAI-compatible chat completions endpoint"""
    if rag_pipeline is None:
        raise HTTPException(status_code=503, detail="RAG pipeline not loaded")
    
    try:
        # Extract the user question from messages
        user_question = None
        for message in reversed(request.messages):
            if message.role == "user":
                user_question = message.content
                break
        
        if not user_question:
            raise HTTPException(status_code=400, detail="No user message found")
        
        # Use the RAG pipeline to generate response
        result = rag_pipeline.invoke({"question": user_question})
        response_text = result.get("answer", "").strip()

        # Extract scenario name from response
        scenario_name = extract_scenario_name(response_text)
        if scenario_name:
            logger.info(f"Detected scenario: {scenario_name}")

        # Format response in OpenAI format
        return ChatCompletionResponse(
            id=f"chatcmpl-{hash(user_question) % 10000000}",
            created=int(__import__("time").time()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }],
            usage={
                "prompt_tokens": len(user_question.split()),
                "completion_tokens": len(response_text.split()),
                "total_tokens": len(user_question.split()) + len(response_text.split())
            },
            scenario_name=scenario_name
        )
    
    except Exception as e:
        logger.error(f"Error generating response: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/completions")
async def completions(request: dict):
    """OpenAI-compatible completions endpoint"""
    if rag_pipeline is None:
        raise HTTPException(status_code=503, detail="RAG pipeline not loaded")
    
    try:
        prompt = request.get("prompt", "")
        
        # Use RAG pipeline for completions too
        result = rag_pipeline.invoke({"question": prompt})
        response_text = result.get("answer", "").strip()
        
        return {
            "id": f"cmpl-{hash(prompt) % 10000000}",
            "object": "text_completion",
            "created": int(__import__("time").time()),
            "model": request.get("model", "llama"),
            "choices": [{
                "text": response_text,
                "index": 0,
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": len(prompt.split()),
                "completion_tokens": len(response_text.split()),
                "total_tokens": len(prompt.split()) + len(response_text.split())
            }
        }
    
    except Exception as e:
        logger.error(f"Error generating completion: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    port = int(os.getenv("PORT", 8080))
    host = os.getenv("HOST", "0.0.0.0")
    
    uvicorn.run(app, host=host, port=port)