# Generated by Claude Sonnet 4
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import uvicorn
from contextlib import asynccontextmanager
import os
import logging
import subprocess
import re
from rag_pipelines.llama31_rag_pipeline import load_llama31_rag_pipeline

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global RAG pipeline instance
rag_pipeline = None

def extract_scenario_name(response_text: str) -> Optional[str]:
    """Extract scenario name from model response"""
    # Only look for explicit SCENARIO: pattern
    scenario_match = re.search(r'SCENARIO:\s*([a-zA-Z0-9_-]+)', response_text)
    if scenario_match:
        scenario_name = scenario_match.group(1)
        logger.info(f"✅ Scenario detected: {scenario_name}")
        return scenario_name

    logger.warning("⚠️ Scenario not found in the response - model did not follow SCENARIO: format")
    return None

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512
    stream: Optional[bool] = False

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, int]
    scenario_name: Optional[str] = None  # krknctl scenario name if detected

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize and cleanup the RAG pipeline"""
    global rag_pipeline
    
    logger.info("Loading RAG pipeline with llama.cpp backend...")
    
    try:
        # Use the same pipeline logic with llama.cpp backend and correct persist_dir
        rag_pipeline = load_llama31_rag_pipeline(
            llm_backend="llamacpp",
            persist_dir="/app/docs_index"
        )
        logger.info("RAG pipeline loaded successfully")
        logger.info("FastAPI service ready to accept requests")
    except Exception as e:
        logger.error(f"Failed to load RAG pipeline: {e}")
        logger.error(f"Exception type: {type(e).__name__}")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        raise
    
    yield
    
    # Cleanup
    if rag_pipeline:
        rag_pipeline = None
        logger.info("RAG pipeline cleaned up")

app = FastAPI(
    title="KRKN Lightspeed API",
    description="FastAPI server compatible with OpenAI API for KRKN chaos engineering assistance",
    version="1.0.0",
    lifespan=lifespan
)

@app.get("/")
async def root():
    return {"message": "KRKN Lightspeed API is running"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "rag_pipeline_loaded": rag_pipeline is not None}

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """OpenAI-compatible chat completions endpoint"""
    if rag_pipeline is None:
        raise HTTPException(status_code=503, detail="RAG pipeline not loaded")
    
    try:
        # Extract the user question from messages
        user_question = None
        for message in reversed(request.messages):
            if message.role == "user":
                user_question = message.content
                break
        
        if not user_question:
            raise HTTPException(status_code=400, detail="No user message found")
        
        # Use the RAG pipeline to generate response
        result = rag_pipeline.invoke({"question": user_question})
        response_text = result.get("answer", "").strip()

        # Extract scenario name from response
        scenario_name = extract_scenario_name(response_text)
        if scenario_name:
            logger.info(f"Detected scenario: {scenario_name}")

        # Format response in OpenAI format
        return ChatCompletionResponse(
            id=f"chatcmpl-{hash(user_question) % 10000000}",
            created=int(__import__("time").time()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }],
            usage={
                "prompt_tokens": len(user_question.split()),
                "completion_tokens": len(response_text.split()),
                "total_tokens": len(user_question.split()) + len(response_text.split())
            },
            scenario_name=scenario_name
        )
    
    except Exception as e:
        logger.error(f"Error generating response: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/completions")
async def completions(request: dict):
    """OpenAI-compatible completions endpoint"""
    if rag_pipeline is None:
        raise HTTPException(status_code=503, detail="RAG pipeline not loaded")
    
    try:
        prompt = request.get("prompt", "")
        
        # Use RAG pipeline for completions too
        result = rag_pipeline.invoke({"question": prompt})
        response_text = result.get("answer", "").strip()
        
        return {
            "id": f"cmpl-{hash(prompt) % 10000000}",
            "object": "text_completion",
            "created": int(__import__("time").time()),
            "model": request.get("model", "llama"),
            "choices": [{
                "text": response_text,
                "index": 0,
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": len(prompt.split()),
                "completion_tokens": len(response_text.split()),
                "total_tokens": len(prompt.split()) + len(response_text.split())
            }
        }
    
    except Exception as e:
        logger.error(f"Error generating completion: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    port = int(os.getenv("PORT", 8080))
    host = os.getenv("HOST", "0.0.0.0")
    
    uvicorn.run(app, host=host, port=port)